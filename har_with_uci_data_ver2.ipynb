{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "har_with_uci_data_ver2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaUQ1VXchAMszm6MQTfr8w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyona-yu/HAR_python/blob/master/har_with_uci_data_ver2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fq-jEWK2Pow"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBWOMSKF5Bbj"
      },
      "source": [
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVH8sr-Bzfdy"
      },
      "source": [
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if device =='cuda':\n",
        "    torch.cuda.manual_seed_all(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2P1VSxm2Q8Z",
        "outputId": "15dd3940-8ed2-4acc-812c-f5e4429af4d0"
      },
      "source": [
        "! git clone https://github.com/hyona-yu/Dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Dataset'...\n",
            "remote: Enumerating objects: 660, done.\u001b[K\n",
            "remote: Total 660 (delta 0), reused 0 (delta 0), pack-reused 660\u001b[K\n",
            "Receiving objects: 100% (660/660), 427.85 MiB | 21.20 MiB/s, done.\n",
            "Resolving deltas: 100% (140/140), done.\n",
            "Checking out files: 100% (626/626), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ykz9hZT2UCj"
      },
      "source": [
        "url = 'Dataset/UCI HAR Dataset/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77oqJNn6j_AR"
      },
      "source": [
        "#time step 128\n",
        "signal = ['body_acc_x_', 'body_acc_y_','body_acc_z_','body_gyro_x_','body_gyro_y_','body_gyro_z_','total_acc_x_','total_acc_y_','total_acc_z_']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmrVibCUk0H5"
      },
      "source": [
        "train_csv = pd.DataFrame()\n",
        "for s in signal:\n",
        "  train_path = str(url + \"train/Inertial Signals/\"+ s + \"train.txt\")\n",
        "  train_csv = pd.concat([train_csv, pd.read_csv(train_path, sep ='\\s+', header = None)], axis = 1)\n",
        "  train_subject = pd.read_csv(url + 'train/subject_train.txt', header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "pOSbmUnYlYbo",
        "outputId": "bf70866d-32b3-4473-94c6-570af7d7f848"
      },
      "source": [
        "train_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.010139</td>\n",
              "      <td>0.009276</td>\n",
              "      <td>0.005066</td>\n",
              "      <td>0.010810</td>\n",
              "      <td>0.004045</td>\n",
              "      <td>0.004757</td>\n",
              "      <td>0.006214</td>\n",
              "      <td>0.003307</td>\n",
              "      <td>0.007572</td>\n",
              "      <td>0.005407</td>\n",
              "      <td>0.006221</td>\n",
              "      <td>0.006895</td>\n",
              "      <td>0.004610</td>\n",
              "      <td>0.007331</td>\n",
              "      <td>0.005078</td>\n",
              "      <td>0.005763</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.005443</td>\n",
              "      <td>0.008241</td>\n",
              "      <td>0.006506</td>\n",
              "      <td>0.006532</td>\n",
              "      <td>0.007422</td>\n",
              "      <td>0.005772</td>\n",
              "      <td>0.006240</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>0.004833</td>\n",
              "      <td>0.005499</td>\n",
              "      <td>0.004341</td>\n",
              "      <td>0.005098</td>\n",
              "      <td>0.004269</td>\n",
              "      <td>0.003045</td>\n",
              "      <td>0.003204</td>\n",
              "      <td>0.004520</td>\n",
              "      <td>0.005127</td>\n",
              "      <td>0.003550</td>\n",
              "      <td>0.004234</td>\n",
              "      <td>0.004469</td>\n",
              "      <td>0.003573</td>\n",
              "      <td>0.005136</td>\n",
              "      <td>...</td>\n",
              "      <td>0.101472</td>\n",
              "      <td>0.101226</td>\n",
              "      <td>0.102653</td>\n",
              "      <td>0.101608</td>\n",
              "      <td>0.100634</td>\n",
              "      <td>0.096142</td>\n",
              "      <td>0.090326</td>\n",
              "      <td>0.089905</td>\n",
              "      <td>0.093117</td>\n",
              "      <td>0.095741</td>\n",
              "      <td>0.094250</td>\n",
              "      <td>0.094481</td>\n",
              "      <td>0.097755</td>\n",
              "      <td>0.099002</td>\n",
              "      <td>0.098523</td>\n",
              "      <td>0.095429</td>\n",
              "      <td>0.094212</td>\n",
              "      <td>0.097735</td>\n",
              "      <td>0.102188</td>\n",
              "      <td>0.103457</td>\n",
              "      <td>0.099610</td>\n",
              "      <td>0.097589</td>\n",
              "      <td>0.098616</td>\n",
              "      <td>0.097882</td>\n",
              "      <td>0.096877</td>\n",
              "      <td>0.097010</td>\n",
              "      <td>0.097582</td>\n",
              "      <td>0.097073</td>\n",
              "      <td>0.097425</td>\n",
              "      <td>0.099341</td>\n",
              "      <td>0.100058</td>\n",
              "      <td>0.098564</td>\n",
              "      <td>0.093177</td>\n",
              "      <td>0.088742</td>\n",
              "      <td>0.090505</td>\n",
              "      <td>0.094843</td>\n",
              "      <td>0.098350</td>\n",
              "      <td>0.100385</td>\n",
              "      <td>0.099874</td>\n",
              "      <td>0.094987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.004550</td>\n",
              "      <td>0.002879</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>0.003305</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.001619</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>-0.000363</td>\n",
              "      <td>-0.000487</td>\n",
              "      <td>-0.000356</td>\n",
              "      <td>-0.000229</td>\n",
              "      <td>-0.000131</td>\n",
              "      <td>-0.000441</td>\n",
              "      <td>-0.001565</td>\n",
              "      <td>-0.000929</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>-0.001236</td>\n",
              "      <td>-0.000911</td>\n",
              "      <td>-0.000435</td>\n",
              "      <td>-0.001177</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.002053</td>\n",
              "      <td>0.002738</td>\n",
              "      <td>0.002743</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>-0.000367</td>\n",
              "      <td>-0.000722</td>\n",
              "      <td>-0.001904</td>\n",
              "      <td>-0.004294</td>\n",
              "      <td>-0.000025</td>\n",
              "      <td>0.005251</td>\n",
              "      <td>0.002490</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>-0.000693</td>\n",
              "      <td>...</td>\n",
              "      <td>0.097527</td>\n",
              "      <td>0.097369</td>\n",
              "      <td>0.098893</td>\n",
              "      <td>0.100536</td>\n",
              "      <td>0.098083</td>\n",
              "      <td>0.097308</td>\n",
              "      <td>0.101762</td>\n",
              "      <td>0.105788</td>\n",
              "      <td>0.104529</td>\n",
              "      <td>0.101473</td>\n",
              "      <td>0.096284</td>\n",
              "      <td>0.087116</td>\n",
              "      <td>0.088206</td>\n",
              "      <td>0.096796</td>\n",
              "      <td>0.096812</td>\n",
              "      <td>0.098964</td>\n",
              "      <td>0.104573</td>\n",
              "      <td>0.103425</td>\n",
              "      <td>0.100734</td>\n",
              "      <td>0.097965</td>\n",
              "      <td>0.099716</td>\n",
              "      <td>0.104411</td>\n",
              "      <td>0.103337</td>\n",
              "      <td>0.103670</td>\n",
              "      <td>0.103047</td>\n",
              "      <td>0.099247</td>\n",
              "      <td>0.100684</td>\n",
              "      <td>0.100973</td>\n",
              "      <td>0.097971</td>\n",
              "      <td>0.095684</td>\n",
              "      <td>0.094537</td>\n",
              "      <td>0.098759</td>\n",
              "      <td>0.101977</td>\n",
              "      <td>0.095360</td>\n",
              "      <td>0.089466</td>\n",
              "      <td>0.095126</td>\n",
              "      <td>0.099496</td>\n",
              "      <td>0.093535</td>\n",
              "      <td>0.089035</td>\n",
              "      <td>0.090612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.003531</td>\n",
              "      <td>0.002285</td>\n",
              "      <td>-0.000420</td>\n",
              "      <td>-0.003738</td>\n",
              "      <td>-0.006706</td>\n",
              "      <td>-0.003148</td>\n",
              "      <td>0.000733</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>-0.000946</td>\n",
              "      <td>-0.006476</td>\n",
              "      <td>-0.003423</td>\n",
              "      <td>-0.000610</td>\n",
              "      <td>-0.002929</td>\n",
              "      <td>-0.001796</td>\n",
              "      <td>0.000956</td>\n",
              "      <td>0.002311</td>\n",
              "      <td>0.002538</td>\n",
              "      <td>0.001180</td>\n",
              "      <td>0.001217</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.002156</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>-0.001506</td>\n",
              "      <td>-0.002105</td>\n",
              "      <td>-0.001494</td>\n",
              "      <td>0.002181</td>\n",
              "      <td>0.002876</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>-0.000392</td>\n",
              "      <td>-0.000569</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>-0.001176</td>\n",
              "      <td>-0.001957</td>\n",
              "      <td>-0.000471</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.003380</td>\n",
              "      <td>0.004829</td>\n",
              "      <td>0.003380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.093599</td>\n",
              "      <td>0.089887</td>\n",
              "      <td>0.083149</td>\n",
              "      <td>0.085446</td>\n",
              "      <td>0.089210</td>\n",
              "      <td>0.092701</td>\n",
              "      <td>0.095387</td>\n",
              "      <td>0.093006</td>\n",
              "      <td>0.089667</td>\n",
              "      <td>0.085096</td>\n",
              "      <td>0.084867</td>\n",
              "      <td>0.090206</td>\n",
              "      <td>0.091938</td>\n",
              "      <td>0.094822</td>\n",
              "      <td>0.096023</td>\n",
              "      <td>0.093216</td>\n",
              "      <td>0.096160</td>\n",
              "      <td>0.096263</td>\n",
              "      <td>0.093678</td>\n",
              "      <td>0.096380</td>\n",
              "      <td>0.093724</td>\n",
              "      <td>0.087680</td>\n",
              "      <td>0.085679</td>\n",
              "      <td>0.086568</td>\n",
              "      <td>0.090691</td>\n",
              "      <td>0.092891</td>\n",
              "      <td>0.093875</td>\n",
              "      <td>0.094094</td>\n",
              "      <td>0.087741</td>\n",
              "      <td>0.082867</td>\n",
              "      <td>0.085343</td>\n",
              "      <td>0.087155</td>\n",
              "      <td>0.084546</td>\n",
              "      <td>0.082166</td>\n",
              "      <td>0.081972</td>\n",
              "      <td>0.081413</td>\n",
              "      <td>0.081936</td>\n",
              "      <td>0.083011</td>\n",
              "      <td>0.082334</td>\n",
              "      <td>0.081487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.001772</td>\n",
              "      <td>-0.001311</td>\n",
              "      <td>0.000388</td>\n",
              "      <td>0.000408</td>\n",
              "      <td>-0.000355</td>\n",
              "      <td>0.000998</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>-0.003149</td>\n",
              "      <td>-0.008882</td>\n",
              "      <td>-0.010483</td>\n",
              "      <td>-0.004482</td>\n",
              "      <td>0.004528</td>\n",
              "      <td>0.008167</td>\n",
              "      <td>0.002929</td>\n",
              "      <td>-0.004487</td>\n",
              "      <td>-0.004717</td>\n",
              "      <td>-0.001637</td>\n",
              "      <td>-0.000097</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.002619</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.005851</td>\n",
              "      <td>0.002579</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>0.002138</td>\n",
              "      <td>0.003519</td>\n",
              "      <td>0.002715</td>\n",
              "      <td>0.002370</td>\n",
              "      <td>0.003299</td>\n",
              "      <td>0.002357</td>\n",
              "      <td>0.001481</td>\n",
              "      <td>-0.000340</td>\n",
              "      <td>-0.002201</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.002706</td>\n",
              "      <td>0.001281</td>\n",
              "      <td>-0.000949</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.004952</td>\n",
              "      <td>...</td>\n",
              "      <td>0.082965</td>\n",
              "      <td>0.081726</td>\n",
              "      <td>0.085902</td>\n",
              "      <td>0.088833</td>\n",
              "      <td>0.086746</td>\n",
              "      <td>0.084692</td>\n",
              "      <td>0.082640</td>\n",
              "      <td>0.084150</td>\n",
              "      <td>0.085247</td>\n",
              "      <td>0.083578</td>\n",
              "      <td>0.083925</td>\n",
              "      <td>0.087735</td>\n",
              "      <td>0.092079</td>\n",
              "      <td>0.090971</td>\n",
              "      <td>0.088221</td>\n",
              "      <td>0.087130</td>\n",
              "      <td>0.084942</td>\n",
              "      <td>0.082944</td>\n",
              "      <td>0.079419</td>\n",
              "      <td>0.076888</td>\n",
              "      <td>0.080105</td>\n",
              "      <td>0.084625</td>\n",
              "      <td>0.087186</td>\n",
              "      <td>0.087998</td>\n",
              "      <td>0.087082</td>\n",
              "      <td>0.085528</td>\n",
              "      <td>0.085230</td>\n",
              "      <td>0.088954</td>\n",
              "      <td>0.093179</td>\n",
              "      <td>0.091213</td>\n",
              "      <td>0.088112</td>\n",
              "      <td>0.087322</td>\n",
              "      <td>0.083738</td>\n",
              "      <td>0.082701</td>\n",
              "      <td>0.084490</td>\n",
              "      <td>0.082785</td>\n",
              "      <td>0.084084</td>\n",
              "      <td>0.085761</td>\n",
              "      <td>0.083275</td>\n",
              "      <td>0.081404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000087</td>\n",
              "      <td>-0.000272</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.003126</td>\n",
              "      <td>0.002284</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.001933</td>\n",
              "      <td>0.002270</td>\n",
              "      <td>0.002247</td>\n",
              "      <td>0.002175</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.001420</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.001911</td>\n",
              "      <td>0.001778</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>-0.000716</td>\n",
              "      <td>-0.001347</td>\n",
              "      <td>-0.000384</td>\n",
              "      <td>-0.000135</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.001470</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>-0.000106</td>\n",
              "      <td>-0.001683</td>\n",
              "      <td>-0.001165</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>-0.000102</td>\n",
              "      <td>-0.001546</td>\n",
              "      <td>-0.003518</td>\n",
              "      <td>-0.003204</td>\n",
              "      <td>-0.002376</td>\n",
              "      <td>-0.001825</td>\n",
              "      <td>-0.001754</td>\n",
              "      <td>-0.002504</td>\n",
              "      <td>-0.002612</td>\n",
              "      <td>...</td>\n",
              "      <td>0.084108</td>\n",
              "      <td>0.083109</td>\n",
              "      <td>0.080163</td>\n",
              "      <td>0.076622</td>\n",
              "      <td>0.078279</td>\n",
              "      <td>0.084254</td>\n",
              "      <td>0.085648</td>\n",
              "      <td>0.081490</td>\n",
              "      <td>0.084675</td>\n",
              "      <td>0.088211</td>\n",
              "      <td>0.086471</td>\n",
              "      <td>0.087503</td>\n",
              "      <td>0.082953</td>\n",
              "      <td>0.079125</td>\n",
              "      <td>0.084738</td>\n",
              "      <td>0.087178</td>\n",
              "      <td>0.085370</td>\n",
              "      <td>0.084911</td>\n",
              "      <td>0.090154</td>\n",
              "      <td>0.093388</td>\n",
              "      <td>0.083022</td>\n",
              "      <td>0.074595</td>\n",
              "      <td>0.079912</td>\n",
              "      <td>0.085177</td>\n",
              "      <td>0.082895</td>\n",
              "      <td>0.084075</td>\n",
              "      <td>0.085648</td>\n",
              "      <td>0.082100</td>\n",
              "      <td>0.086196</td>\n",
              "      <td>0.089715</td>\n",
              "      <td>0.088028</td>\n",
              "      <td>0.090312</td>\n",
              "      <td>0.088713</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.086522</td>\n",
              "      <td>0.081640</td>\n",
              "      <td>0.079652</td>\n",
              "      <td>0.081329</td>\n",
              "      <td>0.085397</td>\n",
              "      <td>0.088816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7347</th>\n",
              "      <td>0.388873</td>\n",
              "      <td>0.618763</td>\n",
              "      <td>0.774067</td>\n",
              "      <td>0.586568</td>\n",
              "      <td>0.217007</td>\n",
              "      <td>-0.027330</td>\n",
              "      <td>-0.171294</td>\n",
              "      <td>-0.218988</td>\n",
              "      <td>-0.137680</td>\n",
              "      <td>0.033094</td>\n",
              "      <td>0.134138</td>\n",
              "      <td>0.068537</td>\n",
              "      <td>0.029367</td>\n",
              "      <td>-0.031832</td>\n",
              "      <td>-0.178822</td>\n",
              "      <td>-0.245202</td>\n",
              "      <td>-0.283081</td>\n",
              "      <td>-0.272425</td>\n",
              "      <td>-0.203703</td>\n",
              "      <td>-0.181702</td>\n",
              "      <td>-0.178996</td>\n",
              "      <td>-0.180241</td>\n",
              "      <td>-0.119776</td>\n",
              "      <td>-0.041093</td>\n",
              "      <td>-0.046574</td>\n",
              "      <td>-0.071318</td>\n",
              "      <td>-0.089548</td>\n",
              "      <td>-0.049388</td>\n",
              "      <td>0.073998</td>\n",
              "      <td>0.206127</td>\n",
              "      <td>0.262677</td>\n",
              "      <td>0.215333</td>\n",
              "      <td>0.229796</td>\n",
              "      <td>0.361772</td>\n",
              "      <td>0.520258</td>\n",
              "      <td>0.650501</td>\n",
              "      <td>0.629610</td>\n",
              "      <td>0.483332</td>\n",
              "      <td>0.310894</td>\n",
              "      <td>0.146430</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.217494</td>\n",
              "      <td>-0.188283</td>\n",
              "      <td>-0.189874</td>\n",
              "      <td>-0.196000</td>\n",
              "      <td>-0.211880</td>\n",
              "      <td>-0.234426</td>\n",
              "      <td>-0.228991</td>\n",
              "      <td>-0.207820</td>\n",
              "      <td>-0.172275</td>\n",
              "      <td>-0.137235</td>\n",
              "      <td>-0.115258</td>\n",
              "      <td>-0.107192</td>\n",
              "      <td>-0.121015</td>\n",
              "      <td>-0.162484</td>\n",
              "      <td>-0.239210</td>\n",
              "      <td>-0.349252</td>\n",
              "      <td>-0.439382</td>\n",
              "      <td>-0.448371</td>\n",
              "      <td>-0.417402</td>\n",
              "      <td>-0.366171</td>\n",
              "      <td>-0.260467</td>\n",
              "      <td>-0.178167</td>\n",
              "      <td>-0.166824</td>\n",
              "      <td>-0.163785</td>\n",
              "      <td>-0.130286</td>\n",
              "      <td>-0.064277</td>\n",
              "      <td>-0.036914</td>\n",
              "      <td>-0.075391</td>\n",
              "      <td>-0.100396</td>\n",
              "      <td>-0.116673</td>\n",
              "      <td>-0.115192</td>\n",
              "      <td>-0.086700</td>\n",
              "      <td>-0.101805</td>\n",
              "      <td>-0.095273</td>\n",
              "      <td>-0.029541</td>\n",
              "      <td>0.026907</td>\n",
              "      <td>0.069472</td>\n",
              "      <td>0.086288</td>\n",
              "      <td>0.099188</td>\n",
              "      <td>0.129060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7348</th>\n",
              "      <td>0.711864</td>\n",
              "      <td>0.709237</td>\n",
              "      <td>0.321368</td>\n",
              "      <td>-0.028921</td>\n",
              "      <td>-0.208107</td>\n",
              "      <td>-0.192802</td>\n",
              "      <td>-0.066754</td>\n",
              "      <td>-0.042285</td>\n",
              "      <td>0.058089</td>\n",
              "      <td>0.106855</td>\n",
              "      <td>-0.038674</td>\n",
              "      <td>-0.105272</td>\n",
              "      <td>-0.134513</td>\n",
              "      <td>-0.131155</td>\n",
              "      <td>-0.136471</td>\n",
              "      <td>-0.171105</td>\n",
              "      <td>-0.116432</td>\n",
              "      <td>-0.126822</td>\n",
              "      <td>-0.129641</td>\n",
              "      <td>-0.142410</td>\n",
              "      <td>-0.255536</td>\n",
              "      <td>-0.259922</td>\n",
              "      <td>-0.212128</td>\n",
              "      <td>-0.133162</td>\n",
              "      <td>-0.036709</td>\n",
              "      <td>-0.035769</td>\n",
              "      <td>-0.053730</td>\n",
              "      <td>-0.071545</td>\n",
              "      <td>-0.040989</td>\n",
              "      <td>0.013037</td>\n",
              "      <td>0.025262</td>\n",
              "      <td>0.038054</td>\n",
              "      <td>0.017321</td>\n",
              "      <td>0.014981</td>\n",
              "      <td>0.041400</td>\n",
              "      <td>0.059505</td>\n",
              "      <td>0.114644</td>\n",
              "      <td>0.194145</td>\n",
              "      <td>0.317047</td>\n",
              "      <td>0.424087</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.145436</td>\n",
              "      <td>-0.135170</td>\n",
              "      <td>-0.117926</td>\n",
              "      <td>-0.123752</td>\n",
              "      <td>-0.157530</td>\n",
              "      <td>-0.199548</td>\n",
              "      <td>-0.212682</td>\n",
              "      <td>-0.183082</td>\n",
              "      <td>-0.170469</td>\n",
              "      <td>-0.166963</td>\n",
              "      <td>-0.133398</td>\n",
              "      <td>-0.098018</td>\n",
              "      <td>-0.067463</td>\n",
              "      <td>-0.070818</td>\n",
              "      <td>-0.116851</td>\n",
              "      <td>-0.216332</td>\n",
              "      <td>-0.331846</td>\n",
              "      <td>-0.290569</td>\n",
              "      <td>-0.216240</td>\n",
              "      <td>-0.269364</td>\n",
              "      <td>-0.252643</td>\n",
              "      <td>-0.210474</td>\n",
              "      <td>-0.232017</td>\n",
              "      <td>-0.175126</td>\n",
              "      <td>-0.094612</td>\n",
              "      <td>-0.005053</td>\n",
              "      <td>0.073006</td>\n",
              "      <td>0.042895</td>\n",
              "      <td>0.035498</td>\n",
              "      <td>0.050375</td>\n",
              "      <td>0.028204</td>\n",
              "      <td>0.045156</td>\n",
              "      <td>0.058035</td>\n",
              "      <td>0.064945</td>\n",
              "      <td>0.089264</td>\n",
              "      <td>0.108987</td>\n",
              "      <td>0.150238</td>\n",
              "      <td>0.199324</td>\n",
              "      <td>0.236369</td>\n",
              "      <td>0.253029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7349</th>\n",
              "      <td>-0.226717</td>\n",
              "      <td>-0.177817</td>\n",
              "      <td>-0.150869</td>\n",
              "      <td>-0.132237</td>\n",
              "      <td>-0.068338</td>\n",
              "      <td>0.007874</td>\n",
              "      <td>0.096481</td>\n",
              "      <td>0.205834</td>\n",
              "      <td>0.441342</td>\n",
              "      <td>0.662930</td>\n",
              "      <td>0.516996</td>\n",
              "      <td>0.163970</td>\n",
              "      <td>-0.111588</td>\n",
              "      <td>-0.242033</td>\n",
              "      <td>-0.161571</td>\n",
              "      <td>-0.057781</td>\n",
              "      <td>0.057996</td>\n",
              "      <td>0.230109</td>\n",
              "      <td>0.114135</td>\n",
              "      <td>-0.067563</td>\n",
              "      <td>-0.048464</td>\n",
              "      <td>-0.126804</td>\n",
              "      <td>-0.205752</td>\n",
              "      <td>-0.222148</td>\n",
              "      <td>-0.245844</td>\n",
              "      <td>-0.211233</td>\n",
              "      <td>-0.236905</td>\n",
              "      <td>-0.237512</td>\n",
              "      <td>-0.210507</td>\n",
              "      <td>-0.204754</td>\n",
              "      <td>-0.104595</td>\n",
              "      <td>-0.045143</td>\n",
              "      <td>-0.029798</td>\n",
              "      <td>-0.010249</td>\n",
              "      <td>0.012481</td>\n",
              "      <td>0.118657</td>\n",
              "      <td>0.196060</td>\n",
              "      <td>0.241251</td>\n",
              "      <td>0.300500</td>\n",
              "      <td>0.376790</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.086958</td>\n",
              "      <td>-0.089363</td>\n",
              "      <td>-0.096627</td>\n",
              "      <td>-0.107349</td>\n",
              "      <td>-0.133789</td>\n",
              "      <td>-0.151909</td>\n",
              "      <td>-0.146643</td>\n",
              "      <td>-0.145323</td>\n",
              "      <td>-0.144709</td>\n",
              "      <td>-0.139999</td>\n",
              "      <td>-0.140984</td>\n",
              "      <td>-0.151710</td>\n",
              "      <td>-0.165605</td>\n",
              "      <td>-0.160335</td>\n",
              "      <td>-0.149881</td>\n",
              "      <td>-0.164953</td>\n",
              "      <td>-0.230757</td>\n",
              "      <td>-0.327586</td>\n",
              "      <td>-0.363994</td>\n",
              "      <td>-0.368852</td>\n",
              "      <td>-0.410020</td>\n",
              "      <td>-0.406715</td>\n",
              "      <td>-0.388785</td>\n",
              "      <td>-0.402327</td>\n",
              "      <td>-0.318985</td>\n",
              "      <td>-0.184458</td>\n",
              "      <td>-0.102218</td>\n",
              "      <td>-0.011336</td>\n",
              "      <td>0.034239</td>\n",
              "      <td>-0.043995</td>\n",
              "      <td>-0.117974</td>\n",
              "      <td>-0.080109</td>\n",
              "      <td>-0.015432</td>\n",
              "      <td>0.031859</td>\n",
              "      <td>0.072452</td>\n",
              "      <td>0.065696</td>\n",
              "      <td>0.079038</td>\n",
              "      <td>0.111295</td>\n",
              "      <td>0.126302</td>\n",
              "      <td>0.188621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7350</th>\n",
              "      <td>-0.064806</td>\n",
              "      <td>-0.079933</td>\n",
              "      <td>-0.068350</td>\n",
              "      <td>-0.038477</td>\n",
              "      <td>-0.028679</td>\n",
              "      <td>0.018335</td>\n",
              "      <td>0.077571</td>\n",
              "      <td>0.253580</td>\n",
              "      <td>0.507805</td>\n",
              "      <td>0.583999</td>\n",
              "      <td>0.466749</td>\n",
              "      <td>0.233940</td>\n",
              "      <td>-0.056077</td>\n",
              "      <td>-0.163528</td>\n",
              "      <td>-0.037607</td>\n",
              "      <td>-0.006132</td>\n",
              "      <td>-0.031113</td>\n",
              "      <td>0.022096</td>\n",
              "      <td>-0.039334</td>\n",
              "      <td>-0.070132</td>\n",
              "      <td>-0.042110</td>\n",
              "      <td>-0.097888</td>\n",
              "      <td>-0.112671</td>\n",
              "      <td>-0.166334</td>\n",
              "      <td>-0.205544</td>\n",
              "      <td>-0.161619</td>\n",
              "      <td>-0.172629</td>\n",
              "      <td>-0.161423</td>\n",
              "      <td>-0.156584</td>\n",
              "      <td>-0.161717</td>\n",
              "      <td>-0.100413</td>\n",
              "      <td>-0.039997</td>\n",
              "      <td>-0.009242</td>\n",
              "      <td>-0.057404</td>\n",
              "      <td>-0.092720</td>\n",
              "      <td>-0.028126</td>\n",
              "      <td>0.050525</td>\n",
              "      <td>0.152816</td>\n",
              "      <td>0.236484</td>\n",
              "      <td>0.245565</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.116897</td>\n",
              "      <td>-0.103137</td>\n",
              "      <td>-0.090091</td>\n",
              "      <td>-0.113315</td>\n",
              "      <td>-0.124259</td>\n",
              "      <td>-0.169151</td>\n",
              "      <td>-0.189740</td>\n",
              "      <td>-0.156570</td>\n",
              "      <td>-0.138032</td>\n",
              "      <td>-0.142260</td>\n",
              "      <td>-0.156863</td>\n",
              "      <td>-0.161927</td>\n",
              "      <td>-0.168393</td>\n",
              "      <td>-0.170999</td>\n",
              "      <td>-0.183511</td>\n",
              "      <td>-0.251072</td>\n",
              "      <td>-0.304773</td>\n",
              "      <td>-0.286929</td>\n",
              "      <td>-0.271133</td>\n",
              "      <td>-0.318102</td>\n",
              "      <td>-0.361370</td>\n",
              "      <td>-0.358647</td>\n",
              "      <td>-0.344486</td>\n",
              "      <td>-0.307890</td>\n",
              "      <td>-0.274540</td>\n",
              "      <td>-0.224581</td>\n",
              "      <td>-0.093530</td>\n",
              "      <td>0.017510</td>\n",
              "      <td>0.012284</td>\n",
              "      <td>-0.036033</td>\n",
              "      <td>-0.032441</td>\n",
              "      <td>-0.001154</td>\n",
              "      <td>0.028541</td>\n",
              "      <td>0.052355</td>\n",
              "      <td>0.040251</td>\n",
              "      <td>0.060450</td>\n",
              "      <td>0.108725</td>\n",
              "      <td>0.128925</td>\n",
              "      <td>0.172251</td>\n",
              "      <td>0.210071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7351</th>\n",
              "      <td>-0.194471</td>\n",
              "      <td>-0.173802</td>\n",
              "      <td>-0.127545</td>\n",
              "      <td>-0.108369</td>\n",
              "      <td>-0.121152</td>\n",
              "      <td>-0.074004</td>\n",
              "      <td>-0.002135</td>\n",
              "      <td>0.177865</td>\n",
              "      <td>0.516936</td>\n",
              "      <td>0.701378</td>\n",
              "      <td>0.523162</td>\n",
              "      <td>0.198638</td>\n",
              "      <td>-0.033735</td>\n",
              "      <td>-0.125064</td>\n",
              "      <td>-0.081785</td>\n",
              "      <td>-0.049427</td>\n",
              "      <td>-0.037120</td>\n",
              "      <td>0.032376</td>\n",
              "      <td>-0.010414</td>\n",
              "      <td>-0.075065</td>\n",
              "      <td>-0.088306</td>\n",
              "      <td>-0.154105</td>\n",
              "      <td>-0.138289</td>\n",
              "      <td>-0.141416</td>\n",
              "      <td>-0.190921</td>\n",
              "      <td>-0.165662</td>\n",
              "      <td>-0.197339</td>\n",
              "      <td>-0.233312</td>\n",
              "      <td>-0.234903</td>\n",
              "      <td>-0.198456</td>\n",
              "      <td>-0.069192</td>\n",
              "      <td>0.004701</td>\n",
              "      <td>0.006175</td>\n",
              "      <td>-0.018244</td>\n",
              "      <td>-0.041329</td>\n",
              "      <td>0.014147</td>\n",
              "      <td>0.102656</td>\n",
              "      <td>0.199744</td>\n",
              "      <td>0.261994</td>\n",
              "      <td>0.266400</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.138260</td>\n",
              "      <td>-0.099591</td>\n",
              "      <td>-0.077675</td>\n",
              "      <td>-0.092547</td>\n",
              "      <td>-0.090676</td>\n",
              "      <td>-0.122148</td>\n",
              "      <td>-0.150857</td>\n",
              "      <td>-0.157814</td>\n",
              "      <td>-0.177368</td>\n",
              "      <td>-0.175800</td>\n",
              "      <td>-0.178127</td>\n",
              "      <td>-0.200512</td>\n",
              "      <td>-0.221387</td>\n",
              "      <td>-0.227013</td>\n",
              "      <td>-0.214444</td>\n",
              "      <td>-0.215596</td>\n",
              "      <td>-0.225710</td>\n",
              "      <td>-0.227692</td>\n",
              "      <td>-0.212477</td>\n",
              "      <td>-0.180903</td>\n",
              "      <td>-0.166418</td>\n",
              "      <td>-0.155898</td>\n",
              "      <td>-0.143926</td>\n",
              "      <td>-0.178004</td>\n",
              "      <td>-0.236032</td>\n",
              "      <td>-0.277397</td>\n",
              "      <td>-0.340383</td>\n",
              "      <td>-0.401423</td>\n",
              "      <td>-0.404786</td>\n",
              "      <td>-0.414488</td>\n",
              "      <td>-0.429072</td>\n",
              "      <td>-0.392275</td>\n",
              "      <td>-0.344507</td>\n",
              "      <td>-0.281695</td>\n",
              "      <td>-0.223023</td>\n",
              "      <td>-0.205803</td>\n",
              "      <td>-0.180733</td>\n",
              "      <td>-0.156105</td>\n",
              "      <td>-0.122798</td>\n",
              "      <td>-0.083572</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7352 rows × 1152 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2    ...       125       126       127\n",
              "0     0.000181  0.010139  0.009276  ...  0.100385  0.099874  0.094987\n",
              "1     0.001094  0.004550  0.002879  ...  0.093535  0.089035  0.090612\n",
              "2     0.003531  0.002285 -0.000420  ...  0.083011  0.082334  0.081487\n",
              "3    -0.001772 -0.001311  0.000388  ...  0.085761  0.083275  0.081404\n",
              "4     0.000087 -0.000272  0.001022  ...  0.081329  0.085397  0.088816\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "7347  0.388873  0.618763  0.774067  ...  0.086288  0.099188  0.129060\n",
              "7348  0.711864  0.709237  0.321368  ...  0.199324  0.236369  0.253029\n",
              "7349 -0.226717 -0.177817 -0.150869  ...  0.111295  0.126302  0.188621\n",
              "7350 -0.064806 -0.079933 -0.068350  ...  0.128925  0.172251  0.210071\n",
              "7351 -0.194471 -0.173802 -0.127545  ... -0.156105 -0.122798 -0.083572\n",
              "\n",
              "[7352 rows x 1152 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "K77lrI95joFP",
        "outputId": "15614d24-cb37-4b03-e408-118c2b773a1f"
      },
      "source": [
        "train_subject"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7347</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7348</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7349</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7350</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7351</th>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7352 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0\n",
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      1\n",
              "4      1\n",
              "...   ..\n",
              "7347  30\n",
              "7348  30\n",
              "7349  30\n",
              "7350  30\n",
              "7351  30\n",
              "\n",
              "[7352 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqXrcvLumEus"
      },
      "source": [
        "test_csv = pd.DataFrame()\n",
        "for s in signal:\n",
        "  test_path = str(url + \"test/Inertial Signals/\"+ s + \"test.txt\")\n",
        "  test_csv = pd.concat([test_csv, pd.read_csv(test_path, sep ='\\s+', header = None)], axis = 1)\n",
        "  test_subject = pd.read_csv(url + \"test/subject_test.txt\", header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "RM6MFTqLmLTS",
        "outputId": "45e224d5-ddb7-42e6-8d8e-6007402f2a2b"
      },
      "source": [
        "test_csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.011653</td>\n",
              "      <td>0.013109</td>\n",
              "      <td>0.011269</td>\n",
              "      <td>0.027831</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>-0.018965</td>\n",
              "      <td>-0.061920</td>\n",
              "      <td>-0.094248</td>\n",
              "      <td>-0.079534</td>\n",
              "      <td>-0.070084</td>\n",
              "      <td>-0.019152</td>\n",
              "      <td>-0.002267</td>\n",
              "      <td>-0.030111</td>\n",
              "      <td>-0.011910</td>\n",
              "      <td>-0.015542</td>\n",
              "      <td>-0.016666</td>\n",
              "      <td>-0.006630</td>\n",
              "      <td>-0.023481</td>\n",
              "      <td>-0.017628</td>\n",
              "      <td>-0.021150</td>\n",
              "      <td>-0.032659</td>\n",
              "      <td>-0.022478</td>\n",
              "      <td>-0.010197</td>\n",
              "      <td>-0.003120</td>\n",
              "      <td>-0.016496</td>\n",
              "      <td>-0.020442</td>\n",
              "      <td>-0.027568</td>\n",
              "      <td>-0.055032</td>\n",
              "      <td>-0.057480</td>\n",
              "      <td>-0.047096</td>\n",
              "      <td>-0.028362</td>\n",
              "      <td>-0.017185</td>\n",
              "      <td>-0.007376</td>\n",
              "      <td>0.024779</td>\n",
              "      <td>0.023929</td>\n",
              "      <td>0.021248</td>\n",
              "      <td>0.009700</td>\n",
              "      <td>-0.035119</td>\n",
              "      <td>-0.010733</td>\n",
              "      <td>0.023672</td>\n",
              "      <td>...</td>\n",
              "      <td>0.160443</td>\n",
              "      <td>0.151523</td>\n",
              "      <td>0.147972</td>\n",
              "      <td>0.154167</td>\n",
              "      <td>0.153757</td>\n",
              "      <td>0.151731</td>\n",
              "      <td>0.146116</td>\n",
              "      <td>0.143020</td>\n",
              "      <td>0.143608</td>\n",
              "      <td>0.144273</td>\n",
              "      <td>0.149575</td>\n",
              "      <td>0.150313</td>\n",
              "      <td>0.149967</td>\n",
              "      <td>0.150430</td>\n",
              "      <td>0.148942</td>\n",
              "      <td>0.149977</td>\n",
              "      <td>0.147962</td>\n",
              "      <td>0.148390</td>\n",
              "      <td>0.150847</td>\n",
              "      <td>0.150729</td>\n",
              "      <td>0.152542</td>\n",
              "      <td>0.151450</td>\n",
              "      <td>0.153751</td>\n",
              "      <td>0.158655</td>\n",
              "      <td>0.158038</td>\n",
              "      <td>0.158718</td>\n",
              "      <td>0.158712</td>\n",
              "      <td>0.159610</td>\n",
              "      <td>0.161821</td>\n",
              "      <td>0.158132</td>\n",
              "      <td>0.155110</td>\n",
              "      <td>0.153346</td>\n",
              "      <td>0.149289</td>\n",
              "      <td>0.147401</td>\n",
              "      <td>0.146905</td>\n",
              "      <td>0.145261</td>\n",
              "      <td>0.143904</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>0.144703</td>\n",
              "      <td>0.145494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.009280</td>\n",
              "      <td>0.004930</td>\n",
              "      <td>0.003954</td>\n",
              "      <td>0.009214</td>\n",
              "      <td>0.016156</td>\n",
              "      <td>0.014079</td>\n",
              "      <td>0.013105</td>\n",
              "      <td>0.021247</td>\n",
              "      <td>0.026823</td>\n",
              "      <td>0.031054</td>\n",
              "      <td>0.037886</td>\n",
              "      <td>0.026463</td>\n",
              "      <td>-0.021441</td>\n",
              "      <td>-0.060568</td>\n",
              "      <td>-0.024725</td>\n",
              "      <td>0.020272</td>\n",
              "      <td>-0.001934</td>\n",
              "      <td>-0.008907</td>\n",
              "      <td>0.009538</td>\n",
              "      <td>-0.004332</td>\n",
              "      <td>-0.001864</td>\n",
              "      <td>0.004955</td>\n",
              "      <td>-0.005336</td>\n",
              "      <td>0.002001</td>\n",
              "      <td>-0.001708</td>\n",
              "      <td>-0.006463</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>-0.003640</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>-0.001442</td>\n",
              "      <td>-0.005959</td>\n",
              "      <td>0.000344</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>0.002381</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>-0.001456</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000570</td>\n",
              "      <td>0.002605</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>...</td>\n",
              "      <td>0.124839</td>\n",
              "      <td>0.125669</td>\n",
              "      <td>0.126854</td>\n",
              "      <td>0.133731</td>\n",
              "      <td>0.140218</td>\n",
              "      <td>0.134717</td>\n",
              "      <td>0.125654</td>\n",
              "      <td>0.119328</td>\n",
              "      <td>0.119537</td>\n",
              "      <td>0.122430</td>\n",
              "      <td>0.120289</td>\n",
              "      <td>0.121756</td>\n",
              "      <td>0.128699</td>\n",
              "      <td>0.132581</td>\n",
              "      <td>0.131261</td>\n",
              "      <td>0.129647</td>\n",
              "      <td>0.133942</td>\n",
              "      <td>0.137049</td>\n",
              "      <td>0.135974</td>\n",
              "      <td>0.138160</td>\n",
              "      <td>0.138747</td>\n",
              "      <td>0.134309</td>\n",
              "      <td>0.127496</td>\n",
              "      <td>0.122937</td>\n",
              "      <td>0.125959</td>\n",
              "      <td>0.129914</td>\n",
              "      <td>0.131720</td>\n",
              "      <td>0.133536</td>\n",
              "      <td>0.134832</td>\n",
              "      <td>0.135538</td>\n",
              "      <td>0.133731</td>\n",
              "      <td>0.132851</td>\n",
              "      <td>0.132054</td>\n",
              "      <td>0.132224</td>\n",
              "      <td>0.139767</td>\n",
              "      <td>0.147877</td>\n",
              "      <td>0.153025</td>\n",
              "      <td>0.152788</td>\n",
              "      <td>0.139843</td>\n",
              "      <td>0.121314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.005732</td>\n",
              "      <td>0.007066</td>\n",
              "      <td>0.005110</td>\n",
              "      <td>0.002434</td>\n",
              "      <td>0.002024</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>0.001930</td>\n",
              "      <td>0.004875</td>\n",
              "      <td>0.003776</td>\n",
              "      <td>0.002400</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>-0.000113</td>\n",
              "      <td>0.000473</td>\n",
              "      <td>0.002405</td>\n",
              "      <td>0.001630</td>\n",
              "      <td>0.002221</td>\n",
              "      <td>0.002135</td>\n",
              "      <td>0.002463</td>\n",
              "      <td>0.003525</td>\n",
              "      <td>0.001243</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.002337</td>\n",
              "      <td>0.004713</td>\n",
              "      <td>0.004801</td>\n",
              "      <td>0.003798</td>\n",
              "      <td>0.003253</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>-0.000773</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>-0.000269</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000772</td>\n",
              "      <td>0.004269</td>\n",
              "      <td>0.007619</td>\n",
              "      <td>0.004565</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000717</td>\n",
              "      <td>-0.000957</td>\n",
              "      <td>-0.003173</td>\n",
              "      <td>...</td>\n",
              "      <td>0.141447</td>\n",
              "      <td>0.138268</td>\n",
              "      <td>0.136213</td>\n",
              "      <td>0.135355</td>\n",
              "      <td>0.136989</td>\n",
              "      <td>0.139679</td>\n",
              "      <td>0.143010</td>\n",
              "      <td>0.146465</td>\n",
              "      <td>0.144218</td>\n",
              "      <td>0.143410</td>\n",
              "      <td>0.143945</td>\n",
              "      <td>0.142914</td>\n",
              "      <td>0.146899</td>\n",
              "      <td>0.150695</td>\n",
              "      <td>0.150838</td>\n",
              "      <td>0.147119</td>\n",
              "      <td>0.142718</td>\n",
              "      <td>0.136946</td>\n",
              "      <td>0.129181</td>\n",
              "      <td>0.134041</td>\n",
              "      <td>0.139226</td>\n",
              "      <td>0.134179</td>\n",
              "      <td>0.133947</td>\n",
              "      <td>0.131320</td>\n",
              "      <td>0.125988</td>\n",
              "      <td>0.125503</td>\n",
              "      <td>0.124250</td>\n",
              "      <td>0.124186</td>\n",
              "      <td>0.124420</td>\n",
              "      <td>0.126557</td>\n",
              "      <td>0.131392</td>\n",
              "      <td>0.132404</td>\n",
              "      <td>0.134938</td>\n",
              "      <td>0.137589</td>\n",
              "      <td>0.134431</td>\n",
              "      <td>0.133174</td>\n",
              "      <td>0.133712</td>\n",
              "      <td>0.132678</td>\n",
              "      <td>0.132694</td>\n",
              "      <td>0.132117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000452</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>-0.002485</td>\n",
              "      <td>-0.004562</td>\n",
              "      <td>-0.006002</td>\n",
              "      <td>-0.006382</td>\n",
              "      <td>-0.005276</td>\n",
              "      <td>-0.001489</td>\n",
              "      <td>0.005153</td>\n",
              "      <td>0.004951</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>-0.003597</td>\n",
              "      <td>-0.007371</td>\n",
              "      <td>-0.006113</td>\n",
              "      <td>-0.007007</td>\n",
              "      <td>-0.006515</td>\n",
              "      <td>-0.005303</td>\n",
              "      <td>-0.005497</td>\n",
              "      <td>-0.004017</td>\n",
              "      <td>-0.001901</td>\n",
              "      <td>0.000642</td>\n",
              "      <td>-0.002360</td>\n",
              "      <td>-0.007317</td>\n",
              "      <td>-0.003625</td>\n",
              "      <td>-0.000644</td>\n",
              "      <td>-0.001341</td>\n",
              "      <td>-0.000848</td>\n",
              "      <td>-0.003763</td>\n",
              "      <td>-0.005992</td>\n",
              "      <td>-0.004094</td>\n",
              "      <td>-0.000786</td>\n",
              "      <td>0.003458</td>\n",
              "      <td>0.003703</td>\n",
              "      <td>-0.000745</td>\n",
              "      <td>-0.003801</td>\n",
              "      <td>-0.002695</td>\n",
              "      <td>-0.000235</td>\n",
              "      <td>-0.000247</td>\n",
              "      <td>-0.000706</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116925</td>\n",
              "      <td>0.124498</td>\n",
              "      <td>0.131239</td>\n",
              "      <td>0.124867</td>\n",
              "      <td>0.123530</td>\n",
              "      <td>0.126676</td>\n",
              "      <td>0.126094</td>\n",
              "      <td>0.128428</td>\n",
              "      <td>0.128808</td>\n",
              "      <td>0.129597</td>\n",
              "      <td>0.133210</td>\n",
              "      <td>0.131184</td>\n",
              "      <td>0.128958</td>\n",
              "      <td>0.129109</td>\n",
              "      <td>0.129185</td>\n",
              "      <td>0.133316</td>\n",
              "      <td>0.137717</td>\n",
              "      <td>0.137781</td>\n",
              "      <td>0.132310</td>\n",
              "      <td>0.121513</td>\n",
              "      <td>0.120037</td>\n",
              "      <td>0.132227</td>\n",
              "      <td>0.139081</td>\n",
              "      <td>0.135362</td>\n",
              "      <td>0.129449</td>\n",
              "      <td>0.126648</td>\n",
              "      <td>0.127680</td>\n",
              "      <td>0.128090</td>\n",
              "      <td>0.127752</td>\n",
              "      <td>0.127270</td>\n",
              "      <td>0.126040</td>\n",
              "      <td>0.124815</td>\n",
              "      <td>0.125353</td>\n",
              "      <td>0.126463</td>\n",
              "      <td>0.121757</td>\n",
              "      <td>0.117611</td>\n",
              "      <td>0.118865</td>\n",
              "      <td>0.116060</td>\n",
              "      <td>0.110997</td>\n",
              "      <td>0.111912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.004362</td>\n",
              "      <td>-0.002765</td>\n",
              "      <td>-0.004905</td>\n",
              "      <td>-0.004682</td>\n",
              "      <td>-0.000267</td>\n",
              "      <td>0.004135</td>\n",
              "      <td>0.005626</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>-0.001542</td>\n",
              "      <td>0.003020</td>\n",
              "      <td>0.007453</td>\n",
              "      <td>0.008221</td>\n",
              "      <td>0.004644</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>-0.001039</td>\n",
              "      <td>-0.004051</td>\n",
              "      <td>-0.006519</td>\n",
              "      <td>-0.004500</td>\n",
              "      <td>-0.001682</td>\n",
              "      <td>-0.002965</td>\n",
              "      <td>-0.002603</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>-0.001433</td>\n",
              "      <td>-0.003978</td>\n",
              "      <td>-0.005538</td>\n",
              "      <td>-0.005916</td>\n",
              "      <td>-0.003250</td>\n",
              "      <td>-0.001155</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.001719</td>\n",
              "      <td>-0.000121</td>\n",
              "      <td>-0.003209</td>\n",
              "      <td>-0.005932</td>\n",
              "      <td>-0.005502</td>\n",
              "      <td>-0.003120</td>\n",
              "      <td>-0.001452</td>\n",
              "      <td>-0.000775</td>\n",
              "      <td>-0.001839</td>\n",
              "      <td>-0.003113</td>\n",
              "      <td>-0.003741</td>\n",
              "      <td>...</td>\n",
              "      <td>0.112746</td>\n",
              "      <td>0.115978</td>\n",
              "      <td>0.115545</td>\n",
              "      <td>0.111981</td>\n",
              "      <td>0.114324</td>\n",
              "      <td>0.112818</td>\n",
              "      <td>0.111041</td>\n",
              "      <td>0.113719</td>\n",
              "      <td>0.112479</td>\n",
              "      <td>0.115182</td>\n",
              "      <td>0.123094</td>\n",
              "      <td>0.127987</td>\n",
              "      <td>0.126593</td>\n",
              "      <td>0.120257</td>\n",
              "      <td>0.117611</td>\n",
              "      <td>0.118280</td>\n",
              "      <td>0.119715</td>\n",
              "      <td>0.121267</td>\n",
              "      <td>0.117990</td>\n",
              "      <td>0.116020</td>\n",
              "      <td>0.119529</td>\n",
              "      <td>0.121717</td>\n",
              "      <td>0.119389</td>\n",
              "      <td>0.114924</td>\n",
              "      <td>0.113130</td>\n",
              "      <td>0.113738</td>\n",
              "      <td>0.114183</td>\n",
              "      <td>0.115353</td>\n",
              "      <td>0.121068</td>\n",
              "      <td>0.129294</td>\n",
              "      <td>0.129091</td>\n",
              "      <td>0.125772</td>\n",
              "      <td>0.126042</td>\n",
              "      <td>0.121086</td>\n",
              "      <td>0.118511</td>\n",
              "      <td>0.125458</td>\n",
              "      <td>0.129365</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>0.122510</td>\n",
              "      <td>0.122760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2942</th>\n",
              "      <td>0.107662</td>\n",
              "      <td>0.124079</td>\n",
              "      <td>0.183420</td>\n",
              "      <td>0.289247</td>\n",
              "      <td>0.393799</td>\n",
              "      <td>0.446456</td>\n",
              "      <td>0.436931</td>\n",
              "      <td>0.363053</td>\n",
              "      <td>0.279962</td>\n",
              "      <td>0.152701</td>\n",
              "      <td>-0.036629</td>\n",
              "      <td>-0.138744</td>\n",
              "      <td>-0.124348</td>\n",
              "      <td>-0.077908</td>\n",
              "      <td>-0.042914</td>\n",
              "      <td>-0.028672</td>\n",
              "      <td>-0.041073</td>\n",
              "      <td>-0.101596</td>\n",
              "      <td>-0.175614</td>\n",
              "      <td>-0.241283</td>\n",
              "      <td>-0.269875</td>\n",
              "      <td>-0.243626</td>\n",
              "      <td>-0.207789</td>\n",
              "      <td>-0.148244</td>\n",
              "      <td>-0.090001</td>\n",
              "      <td>-0.049235</td>\n",
              "      <td>-0.050288</td>\n",
              "      <td>-0.138663</td>\n",
              "      <td>-0.185296</td>\n",
              "      <td>-0.172049</td>\n",
              "      <td>-0.144030</td>\n",
              "      <td>-0.066441</td>\n",
              "      <td>-0.020091</td>\n",
              "      <td>0.018717</td>\n",
              "      <td>0.098879</td>\n",
              "      <td>0.160090</td>\n",
              "      <td>0.220325</td>\n",
              "      <td>0.297836</td>\n",
              "      <td>0.380437</td>\n",
              "      <td>0.499878</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.254714</td>\n",
              "      <td>-0.204254</td>\n",
              "      <td>-0.129771</td>\n",
              "      <td>-0.111090</td>\n",
              "      <td>-0.121199</td>\n",
              "      <td>-0.125646</td>\n",
              "      <td>-0.128441</td>\n",
              "      <td>-0.115726</td>\n",
              "      <td>-0.095156</td>\n",
              "      <td>-0.084208</td>\n",
              "      <td>-0.069504</td>\n",
              "      <td>-0.045775</td>\n",
              "      <td>-0.031137</td>\n",
              "      <td>-0.018302</td>\n",
              "      <td>0.012571</td>\n",
              "      <td>0.027987</td>\n",
              "      <td>0.011513</td>\n",
              "      <td>-0.017839</td>\n",
              "      <td>-0.071446</td>\n",
              "      <td>-0.123978</td>\n",
              "      <td>-0.168950</td>\n",
              "      <td>-0.236523</td>\n",
              "      <td>-0.246141</td>\n",
              "      <td>-0.164307</td>\n",
              "      <td>-0.015036</td>\n",
              "      <td>0.164701</td>\n",
              "      <td>0.100606</td>\n",
              "      <td>-0.185764</td>\n",
              "      <td>-0.319634</td>\n",
              "      <td>-0.352433</td>\n",
              "      <td>-0.386247</td>\n",
              "      <td>-0.358017</td>\n",
              "      <td>-0.328768</td>\n",
              "      <td>-0.343722</td>\n",
              "      <td>-0.408588</td>\n",
              "      <td>-0.402184</td>\n",
              "      <td>-0.351453</td>\n",
              "      <td>-0.330635</td>\n",
              "      <td>-0.254794</td>\n",
              "      <td>-0.229436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2943</th>\n",
              "      <td>-0.042392</td>\n",
              "      <td>-0.017244</td>\n",
              "      <td>0.015269</td>\n",
              "      <td>0.064482</td>\n",
              "      <td>0.086802</td>\n",
              "      <td>0.084482</td>\n",
              "      <td>0.101578</td>\n",
              "      <td>0.130484</td>\n",
              "      <td>0.208370</td>\n",
              "      <td>0.310620</td>\n",
              "      <td>0.406040</td>\n",
              "      <td>0.510686</td>\n",
              "      <td>0.522362</td>\n",
              "      <td>0.445744</td>\n",
              "      <td>0.316129</td>\n",
              "      <td>0.046526</td>\n",
              "      <td>-0.187785</td>\n",
              "      <td>-0.196544</td>\n",
              "      <td>-0.105807</td>\n",
              "      <td>-0.019565</td>\n",
              "      <td>0.024195</td>\n",
              "      <td>0.005535</td>\n",
              "      <td>-0.044931</td>\n",
              "      <td>-0.134279</td>\n",
              "      <td>-0.207694</td>\n",
              "      <td>-0.226039</td>\n",
              "      <td>-0.260966</td>\n",
              "      <td>-0.294753</td>\n",
              "      <td>-0.272075</td>\n",
              "      <td>-0.215169</td>\n",
              "      <td>-0.136506</td>\n",
              "      <td>-0.084216</td>\n",
              "      <td>-0.108601</td>\n",
              "      <td>-0.147826</td>\n",
              "      <td>-0.173658</td>\n",
              "      <td>-0.189847</td>\n",
              "      <td>-0.158803</td>\n",
              "      <td>-0.129845</td>\n",
              "      <td>-0.119675</td>\n",
              "      <td>-0.094032</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.584616</td>\n",
              "      <td>-0.685853</td>\n",
              "      <td>-0.560922</td>\n",
              "      <td>-0.405545</td>\n",
              "      <td>-0.304131</td>\n",
              "      <td>-0.258593</td>\n",
              "      <td>-0.285550</td>\n",
              "      <td>-0.261728</td>\n",
              "      <td>-0.225564</td>\n",
              "      <td>-0.240838</td>\n",
              "      <td>-0.197783</td>\n",
              "      <td>-0.159723</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.118271</td>\n",
              "      <td>-0.162589</td>\n",
              "      <td>-0.216840</td>\n",
              "      <td>-0.221901</td>\n",
              "      <td>-0.205019</td>\n",
              "      <td>-0.135016</td>\n",
              "      <td>-0.064633</td>\n",
              "      <td>-0.049858</td>\n",
              "      <td>-0.054309</td>\n",
              "      <td>-0.024496</td>\n",
              "      <td>-0.012791</td>\n",
              "      <td>-0.051537</td>\n",
              "      <td>-0.055480</td>\n",
              "      <td>-0.057272</td>\n",
              "      <td>-0.096431</td>\n",
              "      <td>-0.181300</td>\n",
              "      <td>-0.298825</td>\n",
              "      <td>-0.303917</td>\n",
              "      <td>-0.239700</td>\n",
              "      <td>-0.231936</td>\n",
              "      <td>-0.227728</td>\n",
              "      <td>-0.193037</td>\n",
              "      <td>-0.151306</td>\n",
              "      <td>-0.191359</td>\n",
              "      <td>-0.355499</td>\n",
              "      <td>-0.500135</td>\n",
              "      <td>-0.489756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2944</th>\n",
              "      <td>-0.147647</td>\n",
              "      <td>-0.169903</td>\n",
              "      <td>-0.168698</td>\n",
              "      <td>-0.138600</td>\n",
              "      <td>-0.144781</td>\n",
              "      <td>-0.072979</td>\n",
              "      <td>-0.040348</td>\n",
              "      <td>-0.023016</td>\n",
              "      <td>-0.025176</td>\n",
              "      <td>-0.074406</td>\n",
              "      <td>-0.061624</td>\n",
              "      <td>-0.049845</td>\n",
              "      <td>-0.008522</td>\n",
              "      <td>0.056958</td>\n",
              "      <td>0.060986</td>\n",
              "      <td>0.063136</td>\n",
              "      <td>0.082882</td>\n",
              "      <td>0.142853</td>\n",
              "      <td>0.208545</td>\n",
              "      <td>0.218749</td>\n",
              "      <td>0.262933</td>\n",
              "      <td>0.402883</td>\n",
              "      <td>0.501852</td>\n",
              "      <td>0.419210</td>\n",
              "      <td>0.321831</td>\n",
              "      <td>0.302616</td>\n",
              "      <td>0.184098</td>\n",
              "      <td>0.024618</td>\n",
              "      <td>-0.054377</td>\n",
              "      <td>-0.095440</td>\n",
              "      <td>-0.102033</td>\n",
              "      <td>-0.102809</td>\n",
              "      <td>-0.118916</td>\n",
              "      <td>-0.147251</td>\n",
              "      <td>-0.204772</td>\n",
              "      <td>-0.250798</td>\n",
              "      <td>-0.262371</td>\n",
              "      <td>-0.253296</td>\n",
              "      <td>-0.215968</td>\n",
              "      <td>-0.141196</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.443611</td>\n",
              "      <td>-0.509548</td>\n",
              "      <td>-0.478118</td>\n",
              "      <td>-0.486231</td>\n",
              "      <td>-0.560368</td>\n",
              "      <td>-0.597858</td>\n",
              "      <td>-0.520692</td>\n",
              "      <td>-0.370584</td>\n",
              "      <td>-0.254734</td>\n",
              "      <td>-0.225487</td>\n",
              "      <td>-0.260446</td>\n",
              "      <td>-0.265528</td>\n",
              "      <td>-0.234353</td>\n",
              "      <td>-0.222743</td>\n",
              "      <td>-0.213553</td>\n",
              "      <td>-0.192937</td>\n",
              "      <td>-0.168708</td>\n",
              "      <td>-0.157731</td>\n",
              "      <td>-0.162184</td>\n",
              "      <td>-0.153456</td>\n",
              "      <td>-0.151518</td>\n",
              "      <td>-0.167772</td>\n",
              "      <td>-0.166744</td>\n",
              "      <td>-0.156651</td>\n",
              "      <td>-0.144423</td>\n",
              "      <td>-0.118029</td>\n",
              "      <td>-0.074986</td>\n",
              "      <td>-0.015655</td>\n",
              "      <td>0.016309</td>\n",
              "      <td>0.010407</td>\n",
              "      <td>0.001781</td>\n",
              "      <td>-0.021745</td>\n",
              "      <td>-0.079452</td>\n",
              "      <td>-0.175389</td>\n",
              "      <td>-0.267170</td>\n",
              "      <td>-0.291177</td>\n",
              "      <td>-0.294972</td>\n",
              "      <td>-0.272397</td>\n",
              "      <td>-0.067734</td>\n",
              "      <td>0.137826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2945</th>\n",
              "      <td>0.071227</td>\n",
              "      <td>-0.088665</td>\n",
              "      <td>-0.070675</td>\n",
              "      <td>-0.116887</td>\n",
              "      <td>-0.214117</td>\n",
              "      <td>-0.219928</td>\n",
              "      <td>-0.275024</td>\n",
              "      <td>-0.229062</td>\n",
              "      <td>-0.245309</td>\n",
              "      <td>-0.242260</td>\n",
              "      <td>-0.171929</td>\n",
              "      <td>-0.224164</td>\n",
              "      <td>-0.156860</td>\n",
              "      <td>-0.114452</td>\n",
              "      <td>-0.123983</td>\n",
              "      <td>-0.047481</td>\n",
              "      <td>-0.034829</td>\n",
              "      <td>0.019015</td>\n",
              "      <td>0.053397</td>\n",
              "      <td>0.064815</td>\n",
              "      <td>0.102783</td>\n",
              "      <td>0.076054</td>\n",
              "      <td>0.111585</td>\n",
              "      <td>0.175587</td>\n",
              "      <td>0.272484</td>\n",
              "      <td>0.428963</td>\n",
              "      <td>0.493073</td>\n",
              "      <td>0.472147</td>\n",
              "      <td>0.416553</td>\n",
              "      <td>0.346866</td>\n",
              "      <td>0.184038</td>\n",
              "      <td>-0.033835</td>\n",
              "      <td>-0.113676</td>\n",
              "      <td>-0.096133</td>\n",
              "      <td>-0.058507</td>\n",
              "      <td>-0.048071</td>\n",
              "      <td>-0.077782</td>\n",
              "      <td>-0.116294</td>\n",
              "      <td>-0.181632</td>\n",
              "      <td>-0.222995</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.230776</td>\n",
              "      <td>-0.218813</td>\n",
              "      <td>-0.238183</td>\n",
              "      <td>-0.303088</td>\n",
              "      <td>-0.376093</td>\n",
              "      <td>-0.425958</td>\n",
              "      <td>-0.419466</td>\n",
              "      <td>-0.473214</td>\n",
              "      <td>-0.621301</td>\n",
              "      <td>-0.683392</td>\n",
              "      <td>-0.526206</td>\n",
              "      <td>-0.283650</td>\n",
              "      <td>-0.224468</td>\n",
              "      <td>-0.282644</td>\n",
              "      <td>-0.264898</td>\n",
              "      <td>-0.239692</td>\n",
              "      <td>-0.227675</td>\n",
              "      <td>-0.203244</td>\n",
              "      <td>-0.220293</td>\n",
              "      <td>-0.229015</td>\n",
              "      <td>-0.190494</td>\n",
              "      <td>-0.136629</td>\n",
              "      <td>-0.095722</td>\n",
              "      <td>-0.105208</td>\n",
              "      <td>-0.127424</td>\n",
              "      <td>-0.133253</td>\n",
              "      <td>-0.165599</td>\n",
              "      <td>-0.197638</td>\n",
              "      <td>-0.199778</td>\n",
              "      <td>-0.187415</td>\n",
              "      <td>-0.156169</td>\n",
              "      <td>-0.128266</td>\n",
              "      <td>-0.083245</td>\n",
              "      <td>-0.015681</td>\n",
              "      <td>-0.017426</td>\n",
              "      <td>-0.038509</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>-0.003512</td>\n",
              "      <td>-0.111786</td>\n",
              "      <td>-0.276068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2946</th>\n",
              "      <td>-0.193642</td>\n",
              "      <td>-0.064987</td>\n",
              "      <td>-0.097122</td>\n",
              "      <td>-0.055735</td>\n",
              "      <td>0.041101</td>\n",
              "      <td>-0.069489</td>\n",
              "      <td>-0.104058</td>\n",
              "      <td>-0.149939</td>\n",
              "      <td>-0.268934</td>\n",
              "      <td>-0.273483</td>\n",
              "      <td>-0.321686</td>\n",
              "      <td>-0.304652</td>\n",
              "      <td>-0.244038</td>\n",
              "      <td>-0.267188</td>\n",
              "      <td>-0.202978</td>\n",
              "      <td>-0.151805</td>\n",
              "      <td>-0.117989</td>\n",
              "      <td>-0.056056</td>\n",
              "      <td>-0.033551</td>\n",
              "      <td>0.023934</td>\n",
              "      <td>0.025335</td>\n",
              "      <td>0.002137</td>\n",
              "      <td>0.062426</td>\n",
              "      <td>0.092002</td>\n",
              "      <td>0.103503</td>\n",
              "      <td>0.134829</td>\n",
              "      <td>0.144563</td>\n",
              "      <td>0.181288</td>\n",
              "      <td>0.276287</td>\n",
              "      <td>0.366257</td>\n",
              "      <td>0.444771</td>\n",
              "      <td>0.518920</td>\n",
              "      <td>0.511258</td>\n",
              "      <td>0.430766</td>\n",
              "      <td>0.225377</td>\n",
              "      <td>-0.097748</td>\n",
              "      <td>-0.197081</td>\n",
              "      <td>-0.093032</td>\n",
              "      <td>-0.098751</td>\n",
              "      <td>-0.092378</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.239171</td>\n",
              "      <td>-0.290206</td>\n",
              "      <td>-0.303726</td>\n",
              "      <td>-0.307908</td>\n",
              "      <td>-0.292706</td>\n",
              "      <td>-0.256507</td>\n",
              "      <td>-0.243051</td>\n",
              "      <td>-0.246092</td>\n",
              "      <td>-0.294946</td>\n",
              "      <td>-0.371723</td>\n",
              "      <td>-0.402845</td>\n",
              "      <td>-0.437000</td>\n",
              "      <td>-0.562383</td>\n",
              "      <td>-0.702058</td>\n",
              "      <td>-0.670948</td>\n",
              "      <td>-0.469834</td>\n",
              "      <td>-0.276469</td>\n",
              "      <td>-0.214648</td>\n",
              "      <td>-0.277310</td>\n",
              "      <td>-0.304957</td>\n",
              "      <td>-0.270843</td>\n",
              "      <td>-0.284196</td>\n",
              "      <td>-0.264903</td>\n",
              "      <td>-0.230607</td>\n",
              "      <td>-0.227316</td>\n",
              "      <td>-0.170396</td>\n",
              "      <td>-0.146518</td>\n",
              "      <td>-0.136173</td>\n",
              "      <td>-0.103573</td>\n",
              "      <td>-0.129311</td>\n",
              "      <td>-0.123657</td>\n",
              "      <td>-0.134904</td>\n",
              "      <td>-0.186747</td>\n",
              "      <td>-0.158777</td>\n",
              "      <td>-0.123668</td>\n",
              "      <td>-0.092933</td>\n",
              "      <td>-0.063138</td>\n",
              "      <td>-0.072539</td>\n",
              "      <td>-0.050975</td>\n",
              "      <td>-0.028925</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2947 rows × 1152 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2    ...       125       126       127\n",
              "0     0.011653  0.013109  0.011269  ...  0.144395  0.144703  0.145494\n",
              "1     0.009280  0.004930  0.003954  ...  0.152788  0.139843  0.121314\n",
              "2     0.005732  0.007066  0.005110  ...  0.132678  0.132694  0.132117\n",
              "3     0.000452  0.000604 -0.002485  ...  0.116060  0.110997  0.111912\n",
              "4    -0.004362 -0.002765 -0.004905  ...  0.125895  0.122510  0.122760\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "2942  0.107662  0.124079  0.183420  ... -0.330635 -0.254794 -0.229436\n",
              "2943 -0.042392 -0.017244  0.015269  ... -0.355499 -0.500135 -0.489756\n",
              "2944 -0.147647 -0.169903 -0.168698  ... -0.272397 -0.067734  0.137826\n",
              "2945  0.071227 -0.088665 -0.070675  ... -0.003512 -0.111786 -0.276068\n",
              "2946 -0.193642 -0.064987 -0.097122  ... -0.072539 -0.050975 -0.028925\n",
              "\n",
              "[2947 rows x 1152 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybJr00SL2VKC"
      },
      "source": [
        "train_y = pd.read_csv(url + 'train/y_train.txt', header = None, names= ['label'])\n",
        "test_y =pd.read_csv(url + 'test/y_test.txt', header =None, names= ['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkyywIenKI4o"
      },
      "source": [
        "# for i in range(len(train_y)):\n",
        "#   if train_y.loc[i,'label'] >=7 :\n",
        "#     train_csv.drop(i, inplace =True)\n",
        "#     train_y.drop(i, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSdh3XUtnHmv"
      },
      "source": [
        "# for i in range(len(test_y)):\n",
        "#   if test_y.loc[i,'label'] >=7 :\n",
        "#     test_csv.drop(i, inplace =True)\n",
        "#     test_y.drop(i, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzmjnIAEP5FS"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
        "encoder = OneHotEncoder()\n",
        "encoder.fit(train_y)\n",
        "train_L = encoder.transform(train_y).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akjZ4UzFUK79"
      },
      "source": [
        "test_L = encoder.transform(test_y).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zcqVCmGFL4g"
      },
      "source": [
        "# scaler = MinMaxScaler()\n",
        "# train_scaled_subject = train_subject#scaler.fit_transform(train_subject)\n",
        "# test_scaled_subject = test_subject#scaler.transform(test_subject)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyQnhgvfkB3s"
      },
      "source": [
        "#(train_scaled_subject)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2yAmhfglQ0-"
      },
      "source": [
        "# sec_train_D = pd.concat([train_csv, pd.DataFrame(train_scaled_subject)], axis= 1)\n",
        "# sec_test_D = pd.concat([test_csv, pd.DataFrame(test_scaled_subject)], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bFYr9yBnAjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx0uklb3qXuD"
      },
      "source": [
        "test_D = np.array(test_csv).reshape(test_csv.shape[0],128,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affPrcutm8Pk"
      },
      "source": [
        "train_D = np.array(train_csv).reshape(train_csv.shape[0], 128, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWrRxoqzF3Oy",
        "outputId": "d3b36919-480c-456d-cad9-a9903ec8da84"
      },
      "source": [
        "train_D.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7352, 128, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuOs53QqDYc0"
      },
      "source": [
        "train_D = torch.FloatTensor(np.array(train_D))\n",
        "train_L = torch.FloatTensor(np.array(train_L))\n",
        "test_D = torch.FloatTensor(np.array(test_D))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v79smq2jtx6L"
      },
      "source": [
        "# sec_train_D = torch.FloatTensor(np.array(sec_train_D))\n",
        "# sec_test_D = torch.FloatTensor(np.array(sec_test_D))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz6BcuUtDZuc"
      },
      "source": [
        "import torch.utils\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_D, train_L), batch_size= 100, shuffle = True, drop_last = True)\n",
        "#sec_train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(sec_train_D, torch.LongTensor(np.array(train_y - 1))), batch_size = 100, shuffle= True, drop_last = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYZktPblW8Dv",
        "outputId": "106322dd-5c0c-49b0-978a-b1fe49267cb3"
      },
      "source": [
        "train_y['label'].value_counts() #14:14:13:13:11:10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6    1407\n",
              "5    1374\n",
              "4    1286\n",
              "1    1226\n",
              "2    1073\n",
              "3     986\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_AV-tEL49gz"
      },
      "source": [
        "#SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuZDS9Mk47iN",
        "outputId": "87478ec6-e4f7-4687-89b7-00817b49aff9"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = SVC(random_state = 42, decision_function_shape= 0.5)\n",
        "params = {\n",
        "    #'C' : [0.5,1,10]\n",
        "}\n",
        "cv = GridSearchCV(clf, param_grid = params)\n",
        "cv.fit(train_csv, train_y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                           class_weight=None, coef0=0.0,\n",
              "                           decision_function_shape=0.5, degree=3, gamma='scale',\n",
              "                           kernel='rbf', max_iter=-1, probability=False,\n",
              "                           random_state=42, shrinking=True, tol=0.001,\n",
              "                           verbose=False),\n",
              "             iid='deprecated', n_jobs=None, param_grid={},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrPPB1yk48rc",
        "outputId": "481bdc46-7f5e-4189-922a-049c9cfb2c13"
      },
      "source": [
        "pred = cv.predict(test_csv)\n",
        "\n",
        "acc = accuracy_score(pred, test_y)\n",
        "print(\"SVM :\",acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM : 0.8873430607397353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCflkIRP4-sT",
        "outputId": "74f42f9e-6a67-4e0a-9585-574308e3ab4e"
      },
      "source": [
        "print(classification_report(test_y, pred)) #support는 정밀도의 평균? "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.87      0.96      0.91       496\n",
            "           2       0.89      0.87      0.88       471\n",
            "           3       0.87      0.86      0.86       420\n",
            "           4       0.85      0.77      0.81       491\n",
            "           5       0.84      0.86      0.85       532\n",
            "           6       1.00      1.00      1.00       537\n",
            "\n",
            "    accuracy                           0.89      2947\n",
            "   macro avg       0.89      0.89      0.88      2947\n",
            "weighted avg       0.89      0.89      0.89      2947\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ddrd_PczwCWs",
        "outputId": "fe649aef-fa18-4778-c37c-beabbe4db78d"
      },
      "source": [
        "report = precision_recall_fscore_support(test_y, pred)\r\n",
        "precision = sum(report[0]*report[-1])/2947\r\n",
        "recall = sum(report[1] * report[-1])/2947\r\n",
        "f1 = sum(report[2]*report[-1])/2947\r\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8871667974820137 0.8873430607397353 0.8865897082003628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsVLcLWYwHk"
      },
      "source": [
        "# PCA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4AYxaA9Yxi7",
        "outputId": "379cff0d-269f-4efc-e00e-d401eda06ad8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(3)\n",
        "pca.fit(np.array(train_csv), np.array(train_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
              "    svd_solver='auto', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL7SRc9s5Wyc"
      },
      "source": [
        "#CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAwSh4fmDTks"
      },
      "source": [
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(CNN, self).__init__()\n",
        "# filter 수 늘리는게 중요? kernel size 크게,\n",
        "\n",
        "        self.layer1 = torch.nn.Sequential(torch.nn.Conv1d(input_size ,128, kernel_size= 5, stride= 1), torch.nn.MaxPool1d(kernel_size= 1), \n",
        "                                          torch.nn.ReLU()) #\n",
        "        self.layer2 = torch.nn.Sequential(torch.nn.Conv1d(128,64, kernel_size= 3, stride= 1, padding =0),\n",
        "                                          torch.nn.MaxPool1d(kernel_size= 1, stride = 1, padding = 0), \n",
        "                                          torch.nn.ReLU())\n",
        "        self.f1 = torch.nn.Flatten()\n",
        "        self.layer3 = torch.nn.Linear(192 , output_size, bias = True)#(input_size - kernel_size + 2*padding_size)/stride + 1\n",
        "        #self.tanh = torch.nn.Tanh()\n",
        "        self.softmax = torch.nn.Softmax(dim = 1)\n",
        "        torch.nn.init.kaiming_uniform(self.layer3.weight)\n",
        "        #torch.nn.init.kaiming_uniform(self.layer4.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        #print(out.shape)\n",
        "        out = self.layer2(out)\n",
        "        #print(out.shape)\n",
        "        #print(out.shape)\n",
        "        out = self.f1(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer3(out)\n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWQ0Z_DzDW3-",
        "outputId": "e7538bb0-3c84-4b09-b5e6-608835b1f1a2"
      },
      "source": [
        "model = CNN(128, train_L.shape[1]).to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP6vk6SpDbE3",
        "outputId": "2472b864-7ebf-441c-ff86-3e4bb9c6da81"
      },
      "source": [
        "running_loss =0\n",
        "for e in range(101):\n",
        "  for i , data in enumerate(train_loader):\n",
        "    x, y = data\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    h = model(x)\n",
        "    loss = criterion(h, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  if (e) % 10 == 0:\n",
        "    print('[%d] loss : %.3f' %(e, running_loss / 100))\n",
        "  running_loss =0\n",
        "\n",
        "print('fin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] loss : 0.049\n",
            "[10] loss : 0.007\n",
            "[20] loss : 0.006\n",
            "[30] loss : 0.007\n",
            "[40] loss : 0.004\n",
            "[50] loss : 0.004\n",
            "[60] loss : 0.003\n",
            "[70] loss : 0.003\n",
            "[80] loss : 0.002\n",
            "[90] loss : 0.002\n",
            "[100] loss : 0.002\n",
            "fin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_gJTnBDDc5F"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  pred=  model(test_D.to(device))\n",
        "  #pred = model(sec_test_D[:,:-1].reshape(-1,128,9).to(device), sec_test_D[:,-1].reshape(-1,1).to(device))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZWBknl5Qf6F",
        "outputId": "e61746c5-eb8f-4744-f73a-d845eb4d4052"
      },
      "source": [
        "train_L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 1., 0.],\n",
              "        ...,\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uug_TSIQ-X0",
        "outputId": "ccd3b1b5-39e2-400e-ee46-59b0ac251ac8"
      },
      "source": [
        "test_y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2947, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHcrpBY1-Qvn",
        "outputId": "3b5da3c5-793b-46b1-c929-25135ae76928"
      },
      "source": [
        "np.array(pred).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2947, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atdZhnKEDf1r",
        "outputId": "38062dfc-5cfc-469f-f9e7-a6a21548f267"
      },
      "source": [
        "acc = [np.argmax(pred.cpu()[i])== np.argmax(test_L[i]) for i in range(len(test_D))]\n",
        "acc = np.array(acc, dtype=np.float32)\n",
        "real_acc = acc.mean()\n",
        "print('CNN : ', real_acc) #0.86 / after 0.88"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN :  0.8802172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs9vwfzNBLzj"
      },
      "source": [
        "prediction = []\r\n",
        "for p in pred:\r\n",
        "  prediction.append(np.argmax(p)+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7iAkXNpyBc9",
        "outputId": "2089eb78-6032-4ac3-8280-a2413c0197d8"
      },
      "source": [
        "report = precision_recall_fscore_support(test_y, prediction)\r\n",
        "precision = sum(report[0]*report[-1])/2947\r\n",
        "recall = sum(report[1] * report[-1])/2947\r\n",
        "f1 = sum(report[2]*report[-1])/2947\r\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8841220558774132 0.8802171700033933 0.8806616111229647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LADqBC0Dn0c"
      },
      "source": [
        "print(classification_report(np.round(pred.cpu()), np.array(test_y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeO21c_qSI-V"
      },
      "source": [
        "#LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDyG3iQPsDVg"
      },
      "source": [
        "train_D[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77y55FdPrP2f"
      },
      "source": [
        "#step size : 128\n",
        "#input = (batch_size, time_steps, input_size)\n",
        "#rnn( input_size, hidden_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lds535bNSJ7V"
      },
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "  def __init__(self, n_class, bi):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.lstm = torch.nn.LSTM(9, 32)\n",
        "    self.lstm2 = torch.nn.LSTM(32, 32)\n",
        "    self.l1 = torch.nn.Linear(128*32, n_class)\n",
        "    self.h1 = (torch.zeros(1,128,32).to(device))\n",
        "    self.c1 = torch.zeros(1,128,32).to(device)\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "    self.relu = torch.nn.ReLU()\n",
        "    self.softmax = torch.nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    #x = x.permute(1,0,2)# 축 바꾸기. 전치행렬처럼\n",
        "    #print(x.shape)\n",
        "    out, hidden = self.lstm(x, (self.c1,self.h1))\n",
        "    #out, _ = self.lstm2(out, hidden)\n",
        "    #print(out.shape)\n",
        "    out = self.relu(out)#tanh(out)\n",
        "    out = self.dropout(out)\n",
        "    out, _  = self.lstm2(out, hidden)\n",
        "    out = self.relu(out)\n",
        "    out = self.dropout(out)\n",
        "    out = out.contiguous().view(-1, 128*32)\n",
        "    #print(out.shape)\n",
        "    model = self.l1(out)\n",
        "    #out = self.softmax(out)\n",
        "    return  model\n",
        "#소프트맥스 지말고 탄젠트나써라"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTIChXTDSMJ5"
      },
      "source": [
        "model = LSTM(train_L.shape[1], True).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA2XCtwRSNDK"
      },
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSOcIVeWSPUe",
        "outputId": "46031a73-fbe1-4c26-be2a-288db9606079"
      },
      "source": [
        "running_loss =0\n",
        "for e in range(201):\n",
        "  for i , data in enumerate(train_loader):\n",
        "    x, y = data\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    h = model(x)\n",
        "    loss = criterion(h, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  if e % 10 == 0:\n",
        "    print('[%d] loss : %.3f' %(e, running_loss / 100))\n",
        "  running_loss =0\n",
        "\n",
        "print('fin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] loss : 0.094\n",
            "[10] loss : 0.026\n",
            "[20] loss : 0.021\n",
            "[30] loss : 0.019\n",
            "[40] loss : 0.018\n",
            "[50] loss : 0.017\n",
            "[60] loss : 0.016\n",
            "[70] loss : 0.015\n",
            "[80] loss : 0.015\n",
            "[90] loss : 0.014\n",
            "[100] loss : 0.013\n",
            "[110] loss : 0.013\n",
            "[120] loss : 0.013\n",
            "[130] loss : 0.013\n",
            "[140] loss : 0.012\n",
            "[150] loss : 0.012\n",
            "[160] loss : 0.012\n",
            "[170] loss : 0.012\n",
            "[180] loss : 0.011\n",
            "[190] loss : 0.011\n",
            "[200] loss : 0.011\n",
            "fin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA9gSys-mwjG",
        "outputId": "a2580399-d9a9-43dd-b714-7e2edd712446"
      },
      "source": [
        "test_D.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2947, 128, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOXGGJ0TSQhr"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  pred_lstm = model(test_D.to(device))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5za2568nTSmY",
        "outputId": "8c9872f6-43ee-41f3-dccc-9382aaf9396e"
      },
      "source": [
        "pred_lstm.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2947, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FsVCdynT2KN",
        "outputId": "3d46528c-5ad1-4078-ca59-b2d3198961e9"
      },
      "source": [
        "train_L.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7352, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a41DwJzSUxZ",
        "outputId": "097b0440-355f-487a-f923-cf6c50ebf74f"
      },
      "source": [
        "acc = [np.argmax(pred_lstm.cpu()[i])== np.argmax(test_L[i]) for i in range(len(test_D))]\n",
        "acc = np.array(acc, dtype=np.float32)\n",
        "real_acc = acc.mean()\n",
        "print('LSTM : ', real_acc) #원래가 0.91 #bi일때 비슷함 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM :  0.9070241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKwwoXV1BkUz"
      },
      "source": [
        "prediction = []\r\n",
        "for p in pred:\r\n",
        "  prediction.append(np.argmax(p)+1)\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO1eta3cBnMd",
        "outputId": "8245bd4d-e994-4514-d826-b46781c412d7"
      },
      "source": [
        "report = precision_recall_fscore_support(test_y, prediction)\r\n",
        "precision = sum(report[0]*report[-1])/2947\r\n",
        "recall = sum(report[1] * report[-1])/2947\r\n",
        "f1 = sum(report[2]*report[-1])/2947\r\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8841220558774132 0.8802171700033933 0.8806616111229647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ko2BMeuelfln"
      },
      "source": [
        "#print(classification_report(np.round(pred_lstm.cpu()), np.array(test_y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcT-pgfbaFkV"
      },
      "source": [
        "#CNN + SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjbRYRA0aHZU"
      },
      "source": [
        "class CNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(CNN, self).__init__()\n",
        "# filter 수 늘리는게 중요? kernel size 크게,\n",
        "\n",
        "        self.layer1 = torch.nn.Sequential(torch.nn.Conv1d(input_size ,128, kernel_size= 5, stride= 1), torch.nn.MaxPool1d(kernel_size= 1), \n",
        "                                          torch.nn.ReLU()) #\n",
        "        self.layer2 = torch.nn.Sequential(torch.nn.Conv1d(128,64, kernel_size= 3, stride= 1, padding =0),\n",
        "                                          torch.nn.MaxPool1d(kernel_size= 1, stride = 1, padding = 0), \n",
        "                                          torch.nn.ReLU())\n",
        "        self.f1 = torch.nn.Flatten()\n",
        "        self.layer3 = torch.nn.Linear(192 , output_size, bias = True) #(input_size - kernel_size + 2*padding_size)/stride + 1\n",
        "        #self.tanh = torch.nn.Tanh()\n",
        "        self.softmax = torch.nn.Softmax(dim =1)\n",
        "        torch.nn.init.kaiming_uniform(self.layer3.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        #print(out.shape)\n",
        "        model = self.layer2(out)\n",
        "        #print(out.shape)\n",
        "        #out = out.view(out.size(0), -1)\n",
        "        model = self.f1(model)\n",
        "        #print(out.shape)\n",
        "        model = self.layer3(model)\n",
        "        model = self.softmax(model)\n",
        "        #model = self.tanh(model)\n",
        "        return out, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItFXyLfXav8X",
        "outputId": "ad8eb748-ad42-4cb4-cbfd-df19db642d85"
      },
      "source": [
        "model = CNN(train_D.shape[1], train_L.shape[1]).to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2htLFNy6aUKt"
      },
      "source": [
        "running_loss = 0\n",
        "new_train = []\n",
        "new_label = []\n",
        "epoch =100\n",
        "for e in range(epoch + 1):\n",
        "  for i , data in enumerate(train_loader):\n",
        "    x, y = data\n",
        "    #print(x.shape)\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    new_x, h = model(x)\n",
        "    loss = criterion(h, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "    if e == epoch:\n",
        "        new_train.extend(new_x.cpu().detach().numpy())\n",
        "        new_label.extend([np.argmax(y[i].cpu().detach().numpy())+1 for i in range(len(y))])\n",
        "  if (e) % 10 == 0:\n",
        "    print('[%d] loss : %.3f' %(e, running_loss / 100))\n",
        "  running_loss =0\n",
        "  \n",
        "\n",
        "print('fin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ9lwGE7pn-u"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  new_test, model_pred=  model(test_D.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVqz8d0jtTl8",
        "outputId": "3b83d3ec-7ed1-4fc4-eff3-fa36bbf87440"
      },
      "source": [
        "new_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2947, 128, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoB9GL44-u7W"
      },
      "source": [
        "for_test = new_test.cpu()#np.resize(new_test.cpu(), (2947, 64*3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Raexy2t6NdzO",
        "outputId": "3aa39f92-6007-4b1a-91b3-2bd2be439eec"
      },
      "source": [
        "np.array(new_train).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7300, 128, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXBTbWffNPNW",
        "outputId": "0004c73d-36d5-4bea-c7e9-ab88c248f9f0"
      },
      "source": [
        "clf = SVC(random_state = 42, decision_function_shape= 0.5)\r\n",
        "params = {\r\n",
        "    #'C' : [0.5,1,10]\r\n",
        "}\r\n",
        "cv = GridSearchCV(clf, param_grid = params)\r\n",
        "cv.fit(np.array(new_train).reshape(7300, 128*5), new_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                           class_weight=None, coef0=0.0,\n",
              "                           decision_function_shape=0.5, degree=3, gamma='scale',\n",
              "                           kernel='rbf', max_iter=-1, probability=False,\n",
              "                           random_state=42, shrinking=True, tol=0.001,\n",
              "                           verbose=False),\n",
              "             iid='deprecated', n_jobs=None, param_grid={},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOCo4PSeptjd"
      },
      "source": [
        "pred = cv.predict(new_test.cpu().reshape(2947, 128*5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdVeFpxotdWd",
        "outputId": "aa26773f-3123-410f-a75f-5546ac9f0698"
      },
      "source": [
        "np.array(pred).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2947,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG4k3NtV7gmP",
        "outputId": "b780de7d-8067-4c95-9564-07965f5684a3"
      },
      "source": [
        "print(classification_report(pred, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.91      0.96      0.93       468\n",
            "           2       0.90      0.90      0.90       472\n",
            "           3       0.98      0.90      0.94       456\n",
            "           4       0.77      0.82      0.79       456\n",
            "           5       0.85      0.81      0.83       558\n",
            "           6       1.00      1.00      1.00       537\n",
            "\n",
            "    accuracy                           0.90      2947\n",
            "   macro avg       0.90      0.90      0.90      2947\n",
            "weighted avg       0.90      0.90      0.90      2947\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ZRoDhnpym2",
        "outputId": "0165dee5-8118-44d5-e117-ed9a2f1bed16"
      },
      "source": [
        "acc = accuracy_score(pred, test_y)\n",
        "print(\"CNN_SVM :\",acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN_SVM : 0.8998982015609094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VPCXaUcTm2D",
        "outputId": "b55e59c4-8e71-4c02-b45d-84200c6fb86f"
      },
      "source": [
        "report = precision_recall_fscore_support(test_y, pred)\n",
        "precision = sum(report[0]*report[-1])/2947\n",
        "recall = sum(report[1] * report[-1])/2947\n",
        "f1 = sum(report[2]*report[-1])/2947\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9003385855774493 0.8998982015609094 0.8994935821126219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2kdJcT55RNq"
      },
      "source": [
        "#LSTM + SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mq-LUp6P5QqV"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "class LSTM(torch.nn.Module):\n",
        "  def __init__(self, n_class, bi):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.lstm = torch.nn.LSTM(128, 32)\n",
        "    self.lstm2 = torch.nn.LSTM(32, 32)\n",
        "    self.h1 = (torch.zeros(1,9,32).to(device))\n",
        "    self.c1 = torch.zeros(1,9,32).to(device)\n",
        "    self.l1 = torch.nn.Linear(9*32, n_class)\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "    self.softmax = torch.nn.Softmax(dim = 1)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    x = x.permute(0,2,1)# 축 바꾸기. 전치행렬처럼\n",
        "    #print(x.shape)\n",
        "    out, hidden = self.lstm(x, (self.h1,self.c1))\n",
        "    out = self.tanh(out)\n",
        "    out = self.dropout(out)\n",
        "    out, _  = self.lstm2(out, hidden)\n",
        "    #print(out.shape)\n",
        "    out = self.tanh(out)\n",
        "    out = self.dropout(out)\n",
        "    #print(out.shape)\n",
        "    model = out.contiguous().view(-1, 9*32)\n",
        "    model = self.l1(model)\n",
        "    #out = self.softmax(out)\n",
        "    return out, model\n",
        "#소프트맥스 지말고 탄젠트나써라"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m35grDGwm5a-"
      },
      "source": [
        "model = LSTM(train_L.shape[1], False).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-HWeR6Mm89O"
      },
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePSqa53ynCac",
        "outputId": "02321368-147a-426c-811f-1c92f3a6bdc6"
      },
      "source": [
        "running_loss = 0\n",
        "epoch = 100\n",
        "new_train = []\n",
        "new_label = []\n",
        "new_subject = []\n",
        "for e in range(epoch+1):\n",
        "  for i , data in enumerate(train_loader):\n",
        "    x,y = data\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    new_t, h = model(x)\n",
        "    loss = criterion(h, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss  += loss.item()\n",
        "    if e == epoch:\n",
        "      new_train.extend(new_t.cpu().detach().numpy())\n",
        "      new_label.append([np.argmax(y[i].cpu().detach().numpy())+1 for i in range(len(y))])\n",
        "      #new_subject.extend(subject.cpu().detach().numpy())\n",
        "  if (e) % 10 == 0:\n",
        "    print('[%d] loss : %.3f' %(e, running_loss / 100))\n",
        "  running_loss = 0\n",
        "\n",
        "print('fin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] loss : 0.092\n",
            "[10] loss : 0.018\n",
            "[20] loss : 0.013\n",
            "[30] loss : 0.012\n",
            "[40] loss : 0.011\n",
            "[50] loss : 0.010\n",
            "[60] loss : 0.009\n",
            "[70] loss : 0.009\n",
            "[80] loss : 0.009\n",
            "[90] loss : 0.008\n",
            "[100] loss : 0.008\n",
            "fin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRILBebx3cPJ"
      },
      "source": [
        "np.array(new_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSyf_StjQ3R0"
      },
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA(9)\n",
        "# pca_train = pca.fit_transform(np.array(new_train).reshape(-1, 128 * 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h-zN557L2bY",
        "outputId": "0b9d69a4-4391-4eef-cd32-5ee63ab24562"
      },
      "source": [
        "np.array(new_train).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7300, 9, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DbsXzeJP2qk"
      },
      "source": [
        "#subject.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L6AtswwMA0k"
      },
      "source": [
        "train_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EswTTenw0LGm"
      },
      "source": [
        "# PATH = './file.pt'\n",
        "# torch.save({\n",
        "#             'epoch': 200,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': loss,\n",
        "\n",
        "#             }, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RqHVoyRPjcV",
        "outputId": "ee3a4dbd-64bc-4fbc-cb93-273fba3ee36e"
      },
      "source": [
        "print(np.array(new_train).reshape(-1,9*32))#.extend(new_subject))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.02568845  0.00642155 -0.         ... -0.13427827  0.01425627\n",
            "  -0.32919732]\n",
            " [-0.0215166   0.02410492  0.02038979 ... -0.15790279  0.01365471\n",
            "  -0.40890953]\n",
            " [ 0.00778214 -0.00607431  0.03027649 ...  0.0078265   0.\n",
            "   0.04006526]\n",
            " ...\n",
            " [ 0.10780137  0.          0.01777333 ... -0.          0.00620736\n",
            "   0.14501512]\n",
            " [ 0.         -0.00839544  0.04295449 ...  0.04972783  0.01377877\n",
            "   0.05056873]\n",
            " [ 0.0046601  -0.01279321  0.         ...  0.02009635  0.0030955\n",
            "   0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x4RH1tIRci-"
      },
      "source": [
        "#check_train = np.append(np.array(new_train).reshape(-1, 128 * 32), np.array(new_subject).reshape(-1,1), axis =1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4Xywp2mR101"
      },
      "source": [
        "#np.array(new_label).reshape(-1,1).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSkQ6WA-np-U",
        "outputId": "7a202fd2-b10e-42c3-ad63-bede20c72118"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "clf = SVC(random_state = 42)#, kernel = 'linear')#class_weight= {1:14,2:14,3:13,4:13,5:11,6:10})#, C = 0.5, kernel = 'linear')\n",
        "params = {\n",
        "#     'C' : [ 0.5,1], 'kernel':['rbf','linear'], 'decision_function_shape':['ova','ovr']\n",
        " }\n",
        "cv = GridSearchCV(clf, param_grid = params)\n",
        "cv.fit(np.array(new_train).reshape(-1,9*32), np.array(new_label).reshape(-1,1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=None, error_score=nan,\n",
              "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                           class_weight=None, coef0=0.0,\n",
              "                           decision_function_shape='ovr', degree=3,\n",
              "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                           probability=False, random_state=42, shrinking=True,\n",
              "                           tol=0.001, verbose=False),\n",
              "             iid='deprecated', n_jobs=None, param_grid={},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaWwIeZCSs3F"
      },
      "source": [
        "cv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HxbBkSV88ju"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  new_test, model_pred=  model(test_D.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh8jXE3In2_k"
      },
      "source": [
        "new_test = np.array(new_test.cpu())#np.resize(new_test, (2996,128))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIXrYIScR9OT"
      },
      "source": [
        "#pca_test = pca.transform(np.array(new_test).reshape(-1, 128 * 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrtLSccijGqq",
        "outputId": "4f88b638-37a0-45eb-e168-dd0266aa3451"
      },
      "source": [
        "np.array(new_test).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2947, 9, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0ONZfVfn6-c"
      },
      "source": [
        "pred = cv.predict(np.asarray(new_test).reshape(-1, 9* 32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H83fo7arn7_c",
        "outputId": "84afa82c-f5dd-433e-9347-05ef1c223051"
      },
      "source": [
        "print(classification_report(pred, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      0.97      0.97       495\n",
            "           2       0.94      0.93      0.94       476\n",
            "           3       0.99      0.94      0.97       443\n",
            "           4       0.76      0.86      0.81       433\n",
            "           5       0.88      0.80      0.84       590\n",
            "           6       0.95      1.00      0.97       510\n",
            "\n",
            "    accuracy                           0.91      2947\n",
            "   macro avg       0.92      0.92      0.91      2947\n",
            "weighted avg       0.92      0.91      0.91      2947\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDrMrswDn94u",
        "outputId": "59f55229-6074-4ae2-e0f4-c69de65c1a7a"
      },
      "source": [
        "acc = accuracy_score(pred, test_y)\n",
        "print(\"LSTM_SVM :\",acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM_SVM : 0.9138106549032915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFrAJwpmoVDo",
        "outputId": "10e3b5f6-cad9-4f02-89b7-30a9ff9c7a22"
      },
      "source": [
        "report = precision_recall_fscore_support(test_y, pred)\n",
        "precision = sum(report[0]*report[-1])/2947\n",
        "recall = sum(report[1] * report[-1])/2947\n",
        "f1 = sum(report[2]*report[-1])/2947\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9156070418856509 0.9138106549032915 0.9135525814184746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va9qRoUb7NNs"
      },
      "source": [
        "#LSTM + CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD2vTtiSwKC_"
      },
      "source": [
        "class LSTM_CNN(torch.nn.Module):\n",
        "  def __init__(self,input_size, n_class, bi ):\n",
        "    super(LSTM_CNN, self).__init__()\n",
        "    self.lstm = torch.nn.LSTM(9, 32, bidirectional = bi)\n",
        "    self.lstm2 = torch.nn.LSTM(32 * (bi+1), 32, bidirectional = bi)\n",
        "    self.h1 = (torch.zeros(1,128,32).to(device))\n",
        "    self.c1 = torch.zeros(1,128,32).to(device)\n",
        "    self.layer1 = torch.nn.Sequential(torch.nn.Conv1d(input_size ,128, kernel_size= 5, stride= 1), torch.nn.MaxPool1d(kernel_size= 1), \n",
        "                                          torch.nn.ReLU()) #\n",
        "    # self.layer2 = torch.nn.Sequential(torch.nn.Conv1d(64,128, kernel_size= 3, stride= 1, padding =0),\n",
        "    #                                       torch.nn.MaxPool1d(kernel_size= 1, stride = 1, padding = 0), \n",
        "    #                                       torch.nn.ReLU())\n",
        "    self.f1 = torch.nn.Flatten()\n",
        "    self.layer3 = torch.nn.Linear(3584, n_class, bias = True) #(input_size - kernel_size + 2*padding_size)/stride + 1 # 26\n",
        "    #self.tanh = torch.nn.Tanh()\n",
        "    self.softmax = torch.nn.Softmax(dim =1)\n",
        "    torch.nn.init.kaiming_uniform(self.layer3.weight)\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "    self.tanh = torch.nn.Tanh()\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    #x = x.permute(1,0,2)# 축 바꾸기. 전치행렬처럼\n",
        "    #print(x.shape)\n",
        "    out, hidden = self.lstm(x, (self.h1,self.c1))\n",
        "    out = self.tanh(out)\n",
        "    out = self.dropout(out)\n",
        "    #print(out.shape)\n",
        "    out, _  = self.lstm2(out, hidden)\n",
        "    #print(out.shape)\n",
        "    out = self.tanh(out)\n",
        "    out = self.dropout(out)\n",
        "    out = self.layer1(out)\n",
        "    #out = self.layer2(out)\n",
        "    out = self.f1(out)\n",
        "    #print(out.shape)\n",
        "    out = self.layer3(out)\n",
        "    #print(out.shape)\n",
        "    out = self.softmax(out)\n",
        "    return out\n",
        "#소프트맥스 지말고 탄젠트나써라"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dbLWt4CTXT1",
        "outputId": "b8aec16c-b44c-4784-e946-014b76e33040"
      },
      "source": [
        "model = LSTM_CNN(train_D.shape[1], train_L.shape[1], bi = False).to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLG3qMUYIYNV",
        "outputId": "32554c34-b70e-419b-d6c1-39e917bfd1e7"
      },
      "source": [
        "train_D.shape[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2J2ubgicTaeV"
      },
      "source": [
        "criterion = torch.nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWwEzMd0TdZR",
        "outputId": "0d50d81e-2d44-4a05-ee46-c06221d13613"
      },
      "source": [
        "running_loss = 0\n",
        "epoch = 100\n",
        "new_train = []\n",
        "new_label = []\n",
        "for e in range(epoch+1):\n",
        "  for i , data in enumerate(train_loader):\n",
        "    x, y = data\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    h = model(x)\n",
        "    loss = criterion(h, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss  += loss.item()\n",
        "  \n",
        "  if (e) % 10 == 0:\n",
        "    print('[%d] loss : %.3f' %(e, running_loss / 100))\n",
        "  running_loss = 0\n",
        "\n",
        "print('fin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0] loss : 0.084\n",
            "[10] loss : 0.008\n",
            "[20] loss : 0.006\n",
            "[30] loss : 0.005\n",
            "[40] loss : 0.004\n",
            "[50] loss : 0.003\n",
            "[60] loss : 0.003\n",
            "[70] loss : 0.002\n",
            "[80] loss : 0.001\n",
            "[90] loss : 0.001\n",
            "[100] loss : 0.001\n",
            "fin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8LapCYX1vKx-"
      },
      "source": [
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  lstm_cnn_pred = model(test_D.to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ydVHXDekvV8_",
        "outputId": "75871502-01ea-4108-c72c-0c640d6aac5f"
      },
      "source": [
        "acc = [np.argmax(lstm_cnn_pred.cpu()[i])== np.argmax(test_L[i]) for i in range(len(test_D))]\n",
        "acc = np.array(acc, dtype=np.float32)\n",
        "real_acc = acc.mean()\n",
        "print('LSTM_CNN : ', real_acc) # 0.90 #bi 0.91 #bi crossentropy 0.89 #bi + CNN1층만 0.92"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM_CNN :  0.91991854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y85yFIY6jZrT",
        "outputId": "8a76f022-dfbc-4123-de5d-1b968c2af9a9"
      },
      "source": [
        "report = precision_recall_fscore_support(test_y, pred)\r\n",
        "precision = sum(report[0]*report[-1])/2947\r\n",
        "recall = sum(report[1] * report[-1])/2947\r\n",
        "f1 = sum(report[2]*report[-1])/2947\r\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9156070418856509 0.9138106549032915 0.9135525814184746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb2y85Ezo630"
      },
      "source": [
        "# attention lstm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDIz2phui0-h"
      },
      "source": [
        "class AT_LSTM(torch.nn.Module):\n",
        "  def __init__(self, output_size):\n",
        "    super( AT_LSTM, self).__init__()\n",
        "    self.lstm1 = torch.nn.LSTM(9, 32)\n",
        "    self.lstm2 = torch.nn.LSTM(32,32)\n",
        "    self.l1 = torch.nn.Linear(32, output_size)\n",
        "    self.h1 = torch.zeros(1,128,32).to(device)\n",
        "    self.c1 = torch.zeros(1,128,32).to(device)\n",
        "\n",
        "  def attention(self, lstm_output, final_state):\n",
        "    hidden = final_state.squeeze(0)\n",
        "    att_w = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n",
        "    soft_att_w = torch.nn.functional.softmax(att_w, 1)\n",
        "    new_hidden = torch.bmm(lstm_output.transpose(1,2), soft_att_w.unsqueeze(2))\n",
        "    return new_hidden\n",
        "\n",
        "  def forward(self, x):\n",
        "    out, hidden = self.lstm1(x, (self.h1, self.c1))\n",
        "    hidden_2 = self.attention(out,hidden[0])\n",
        "    print(hidden_2.shape)\n",
        "    out = out.permute(1,0,2)\n",
        "    out = self.lstm2(out, hidden_2)\n",
        "    out = self.l1(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B05k-oIrETk"
      },
      "source": [
        "model =AT_LSTM(train_L.shape[1]).to(device)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BLiom6Crkpr"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah_7wnw-q3S6"
      },
      "source": [
        "running_loss = 0\n",
        "epoch = 100\n",
        "\n",
        "for e in range(epoch+1):\n",
        "  for i , data in enumerate(train_loader):\n",
        "    x,y = data\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    h = model(x)\n",
        "    loss = criterion(h, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss  += loss.item()\n",
        "\n",
        "  if (e) % 10 == 0:\n",
        "    print('[%d] loss : %.3f' %(e, running_loss / 100))\n",
        "  running_loss = 0\n",
        "\n",
        "print('fin')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}